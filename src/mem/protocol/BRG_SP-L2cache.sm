//-----------------------------------------------------------------------------
// SP_LLC-cache.sm
//-----------------------------------------------------------------------------
// This models LLC cache
//
// Author: Tuan Ta
// Date  : 19/07/03

machine(MachineType:L2Cache, "SP LLC cache")
    : CacheMemory* cacheMemory;
      Cycles cache_resp_latency := 1;
      Cycles to_memory_controller_latency := 1;
      Cycles mem_to_cpu_latency := 1;

      MessageBuffer* requestToLLC,    network = "From", virtual_network = "1", vnet_type = "request";
      MessageBuffer* responseFromLLC, network = "To",   virtual_network = "0", vnet_type = "response";
      MessageBuffer* responseFromMemory;

      int meshDimX := 2;
      int meshDimY := 2;

      // how wide the network is in words
      int netWidth := 1;
      
{
  //---------------------------------------------------------------------------
  // Global state... registers in this mem controller
  // These dont' seem to get initialized to the given values...
  //---------------------------------------------------------------------------
  
  // counters in the mem controller
  // cpu-side and mem-side are allowed to send one vec resp per-cycle
  int _vecCntCpuSide, default=0;
  int _vecCntMemSide, default=0;

  Tick _lastVecRespTimeCpuSide, default=0;
  Tick _lastVecRespTimeMemSide, default=0;

  Addr _lastServicedVecAddrCpuSide, default=0;
  Addr _lastServicedVecAddrMemSide, default=0;

  // only uses contructor when its a TBETable...
    //TBETable lloool, template="<L2Cache_TBE>", constructor="0";
  
  //---------------------------------------------------------------------------
  // States
  //---------------------------------------------------------------------------

  state_declaration(State, desc = "", default = "L2Cache_State_I") {
    I,     AccessPermission:Invalid,     desc = "Not present/invalid";
    IV,    AccessPermission:Busy,        desc = "Waiting for data";
    IM,    AccessPermission:Busy,        desc = "Waiting for data";
    V,     AccessPermission:Read_Write,  desc = "Valid & clean";
    M,     AccessPermission:Read_Write,  desc = "Valid & dirty";
    VV,    AccessPermission:Read_Write,  desc = "Valid & clean. Being read out by vec request";
    VM,    AccessPermission:Read_Write,  desc = "Valid & dirty. Being read out by vec request";
    MI,    AccessPermission:Busy,        desc = "Waiting for writeback ACK";
    IV_LL, AccessPermission:Busy,        desc = "Waiting for data (LL request)";
    IA,    AccessPermission:Busy,        desc = "Waiting for data (ATOMIC request)";
  }

  //---------------------------------------------------------------------------
  // Events
  //---------------------------------------------------------------------------

  enumeration(Event, desc = "Cache events") {
    // from scratchpads
    READ,   desc = "A READ request arrives";
    VREAD0, desc = "The initial vec read";
    VREAD1, desc = "The middle vec read(s)";
    VREAD2, desc = "The last vec read";
    SPLOAD, desc = "Store to spad instead of CPU";
    WRITE,  desc = "A WRITE request arrives";
    LL,     desc = "A LL request arrives";
    SC,     desc = "A SC request arrives";
    ATOMIC, desc = "An ATOMIC request arrives";
    Repl,   desc = "Replacement";
    STALL,  desc = "Just stall for the cycle";
    VEC_STALL_WAIT, desc = "Stall wait the message due to vector cpu port";
    VEC_STALL_WAIT_MEM, desc = "Stall wait the message due to vector mem port";
    // from memory
    Memory_Data,  desc = "Fetched data from memory arrives";
    VData0,       desc = "Fetched data from memory arrives and do vec op"; // kinda hacky
    VData1,       desc = "Fetched data from memory arrives and do vec op";
    VData2,       desc = "Fetched data from memory arrives and do vec op";
    SP_Mem_Data,  desc = "Fetched data from memory arrives and single sp load";
    Memory_Ack,   desc = "Writeback Ack from memory arrives";
  }

  //---------------------------------------------------------------------------
  // Cache entry
  //---------------------------------------------------------------------------

  structure(Entry, desc = "...", interface = "AbstractCacheEntry") {
    State CacheState,     desc = "Cache state";
    DataBlock DataBlk,    desc = "Data in the block";
    WriteMask writeMask,  desc = "Dirty byte mask";
    MachineID LLSC_owner, desc = "Owner of LLSC lock";
  }

  //---------------------------------------------------------------------------
  // TBE entry
  //---------------------------------------------------------------------------

  // Data stored from a packet once taken out of RequestBuf. Needed to finish req when mem resp arrives
  structure(TBE, desc = "...") {
    State     TBEState,   desc = "Transient state";
    DataBlock DataBlk,    desc = "Data for the block";
    WriteMask writeMask,  desc = "Dirty byte mask";
    MachineID Requestor,  desc = "Requestor's ID";
    int       SeqNum,     desc = "Sequence number";
    int       XDim,       desc = "X dimension to access for";
    int       YDim,       desc = "Y dimension to access for";
    Addr      WordAddress,desc = "Word address of element request";
    Addr      PrefetchAddress, desc = "Prefetch address, same wire as datablk";
    int       CoreOffset, desc = "Offset from core origin to start";
    int       SubCoreOffset, desc = "Offset from first response to core";
    int       CountPerCore, desc = "Number of resp per core";
    int       RespCnt,    desc = "The number of resps to provide for this pending req";
    int       PrefetchConfig, desc = "Extra info about how to do wide load";
    // int       Epoch,      desc = "Epoch of the requester";
    LLCRequestType InType, desc = "Keep track of input event to know what to do when get mem";
  }

  structure(TBETable, external = "yes") {
    TBE lookup(Addr);
    void allocate(Addr);
    void deallocate(Addr);
    bool isPresent(Addr);
    bool areNSlotsAvailable(int, Tick);
  }

  //---------------------------------------------------------------------------
  // Structures
  //---------------------------------------------------------------------------

  TBETable TBEs, template="<L2Cache_TBE>", constructor="m_number_of_TBEs";

  //---------------------------------------------------------------------------
  // Prototypes
  //---------------------------------------------------------------------------

  Tick clockEdge();
  Cycles ticksToCycles(Tick t);
  void set_cache_entry(AbstractCacheEntry a);
  void unset_cache_entry();
  void set_tbe(TBE b);
  void unset_tbe();
  void wakeUpAllBuffers();

  //---------------------------------------------------------------------------
  // Functions
  //---------------------------------------------------------------------------

  Entry getCacheEntry(Addr address), return_by_pointer="yes" {
    return static_cast(Entry, "pointer", cacheMemory.lookup(address));
  }

  State getState(TBE tbe, Entry cache_entry, Addr addr) {
    if (is_valid(tbe)) {
      return tbe.TBEState;
    } else if (is_valid(cache_entry)) {
      return cache_entry.CacheState;
    } else {
      return State:I;
    }
  }

  void setState(TBE tbe, Entry cache_entry, Addr addr, State state) {
    if (is_valid(tbe)) {
      tbe.TBEState := state;
    }

    if (is_valid(cache_entry)) {
      cache_entry.CacheState := state;
    }
  }

  AccessPermission getAccessPermission(Addr addr) {
    TBE tbe := TBEs[addr];

    if (is_valid(tbe)) {
      return L2Cache_State_to_permission(tbe.TBEState);
    }

    Entry cache_entry := getCacheEntry(addr);

    if (is_valid(cache_entry)) {
      return L2Cache_State_to_permission(cache_entry.CacheState);
    }

    return AccessPermission:NotPresent;
  }

  void setAccessPermission(Entry cache_entry, Addr addr, State state) {
    if (is_valid(cache_entry)) {
      cache_entry.changePermission(L2Cache_State_to_permission(state));
    }
  }

  bool functionalRead(Addr addr, Packet *pkt) {
    TBE tbe := TBEs[addr];
    Entry cache_entry := getCacheEntry(addr);

    if (is_valid(tbe)) {
      return testAndRead(addr, tbe.DataBlk, pkt);
    } else if (is_valid(cache_entry)) {
      return testAndRead(addr, getCacheEntry(addr).DataBlk, pkt);
    } else {
      functionalMemoryRead(pkt);
      return true;
    }
  }

  int functionalWrite(Addr addr, Packet *pkt) {
    int num_functional_writes := 0;
    TBE tbe := TBEs[addr];
    Entry cache_entry := getCacheEntry(addr);

    if (is_valid(tbe)) {
      num_functional_writes := num_functional_writes +
                        testAndWrite(addr, tbe.DataBlk, pkt);
    } else if (is_valid(cache_entry)) {
      num_functional_writes := num_functional_writes +
                        testAndWrite(addr, getCacheEntry(addr).DataBlk, pkt);
    } else if (version == intToID(0)) {
      // only one L2 bank needs to do the functional write to memory, so let
      // the first L2 do it.
      num_functional_writes := num_functional_writes +
                        functionalMemoryWrite(pkt);
    }

    return num_functional_writes;
  }

  bool isCacheEntryPresent(Addr addr) {
    return cacheMemory.isTagPresent(addr);
  }
  
  // int getLinearIdx(int xDim, int yDim) {
  //   return _vecCnt;
  // }

  // int mod(int a, int b) {
  //   return a - (a / b) * b;
  // }

  // figure out how many cores to serve
  // config 0 - ONE
  // config 1 - ALL
  int getNumCoresToServe(int coreOffset, int vecDimX, int vecDimY, int config) {
    if (config == 0) {
      return 1;
    }
    else if (config == 1) {
      return (vecDimX * vecDimY) - coreOffset;
    }
    else {
      return 1;
    }
  }

  int getTotalResp(int coreOffset, int subCoreOffset, Addr wordAddress, 
      Addr lineAddress, int vecDimX, int vecDimY, int countPerCore, int config
      ) {
    int uint32_size := 4;
    int maxResps := (addressToInt(wordAddress) - addressToInt(lineAddress)) / uint32_size;

    // total
    int maxCores := 0;
    if (config == 0) {
      maxCores := 1;
    }
    else {
      maxCores := vecDimX * vecDimY - coreOffset;
    }
    int totalReqs := (maxCores - 1) * countPerCore + (countPerCore - subCoreOffset);

    if (maxResps > totalReqs) {
      return totalReqs;
    }
    else {
      return maxResps;
    }

  }


  // get the count we want to use depending on which port in progress
  int getPortVecCnt(bool isCpuSide) {
    int vecCnt := _vecCntCpuSide;
    if (!isCpuSide) {
      vecCnt := _vecCntMemSide;
    }
    return vecCnt;
  }

  void setPortVecCnt(int val, bool isCpuSide) {
    if (isCpuSide) {
      _vecCntCpuSide := val;
    }
    else {
      _vecCntMemSide := val;
    }
  }

  Tick getPortLastVecRespTime(bool isCpuSide) {
    if (isCpuSide) {
      return _lastVecRespTimeCpuSide;
    }
    else {
      return _lastVecRespTimeMemSide;
    }
  }

  void setPortLastVecRespTime(Tick val, bool isCpuSide) {
    if (isCpuSide) {
      _lastVecRespTimeCpuSide := val;
    }
    else {
      _lastVecRespTimeMemSide := val;
    }
  }

  Addr getPortLastServVecAddr(bool isCpuSide) {
    if (isCpuSide) {
      return _lastServicedVecAddrCpuSide;
    }
    else {
      return _lastServicedVecAddrMemSide;
    }
  }

  void setPortLastServVecAddr(Addr val, bool isCpuSide) {
    if (isCpuSide) {
      _lastServicedVecAddrCpuSide := val;
    }
    else {
      _lastServicedVecAddrMemSide := val;
    }
  }

  // get the core we want to send the current response to
  // can linearize with respect to x or y and get diff sends
  // currently doing by x first
  int getCoreIdx(int coreIdx, int coreOffset, int subCoreOffset, 
      int vecDimX, int vecDimY, int countPerCore, bool isCpuSide) {
    // get the starting core idx, always 0 in our encoding (no offset for DA core)
    int startingCoreIdx := coreIdx;

    // get the count we want to use depending on which port in progress
    int vecCnt := getPortVecCnt(isCpuSide);

    // // add flat offset to vec cnt
    // int vecFlat := coreOffset;
    // // only go to next vector cores if spatial load
    // if (!isVerticalLoad(config)) {
    //   vecFlat := vecFlat + vecCnt;
    // }

    // the vector core in the group we want to send to
    // consider core offset, and how many words have been sent divided by resp per core
    int vecFlat := coreOffset + ( subCoreOffset + vecCnt ) / countPerCore;

    // get the x y of the current vec count
    int vecX := mod(vecFlat, vecDimX);
    int vecY := vecFlat / vecDimX;

    // linearize vec count with respect to mesh
    int meshVecFlat := vecX + vecY * meshDimX;

    // linearize on mesh cores
    int respIdx := startingCoreIdx + meshVecFlat;
    return respIdx;
  }

  // get the memory address we want to send
  int getWordIdx(Addr lineAddress, Addr wordAddress, bool isCpuSide) {
      int wordAddr := addressToInt(wordAddress); // cast to int so can subtract
      int lineAddr := addressToInt(lineAddress);
      
      // get the count we want to use depending on which port in progress
      int vecCnt := getPortVecCnt(isCpuSide);

      // get offset IN WORDS, need to divide by word size
      int uint32_t_size := 4;
      int wordOffset := ((wordAddr - lineAddr) / uint32_t_size) + vecCnt;
      return wordOffset;
  }

  // get the scratchpad address to send the response to
  Addr getSpadIdx(Addr baseSpadAddr, int coreToServer, int subCoreOffset, 
      int countPerCore, bool isCpuSide) {
    int spadIdx := addressToInt(baseSpadAddr) + (coreToServer << 12);

    // get the count we want to use depending on which port in progress
    int vecCnt := getPortVecCnt(isCpuSide);

    // move up count
    // if (isVerticalLoad(config)) {
    //   int uint32_t_size := 4;
    //   spadIdx := spadIdx + (vecCnt * uint32_t_size);
    // }
    int uint32_t_size := 4;
    int spadOffset := mod((subCoreOffset + vecCnt), countPerCore); 
    spadIdx := spadIdx + (spadOffset * uint32_t_size);
    return intToAddress(spadIdx);
  }

  int getNumWordsInResp(int countPerCore, int subCoreOffset, bool isCpuSide) {
    if (countPerCore > 1) {
      int vecCnt := getPortVecCnt(isCpuSide);
      int respForCore := mod((subCoreOffset + vecCnt), countPerCore);
      int remCnt := countPerCore - respForCore;
      // do min of respCnt and netWidth
      if (remCnt > netWidth) {
        return netWidth;
      }
      else {
        return remCnt;
      }
    }
    else {
      return 1;
    }
  }

  bool isFirstLoadOfVec(bool isCpuSide) {
    int vecCnt := getPortVecCnt(isCpuSide);
    return vecCnt == 0;
  }

  bool isLastLoadOfVec(int count, int countPerCore, int subCoreOffset, bool isCpuSide) {
    int vecCnt := getPortVecCnt(isCpuSide);
    int nextVecCnt := vecCnt + 
      getNumWordsInResp(countPerCore, subCoreOffset, isCpuSide);
    return (nextVecCnt == count);
  }

  void increaseVecCount(int respCnt, int countPerCore, int subCoreOffset, bool isCpuSide) {
    int vecCnt := getPortVecCnt(isCpuSide) + 
      getNumWordsInResp(countPerCore, subCoreOffset, isCpuSide);
    setPortVecCnt(vecCnt, isCpuSide);
    DPRINTF(Frame, "increase vec cnt to %d\n", getPortVecCnt(isCpuSide));
  }

  void resetVecCntr(bool isCpuSide) {
    setPortVecCnt(0, isCpuSide);
    DPRINTF(Frame, "reset vec cnt\n");
  }
  
  bool isVector(int count) {
    // return (xDim > 1 && yDim > 1);
    return count > 1;
  }

  bool requiresMultipleResps(int totalCount, int countPerCore, int subCoreOffset, bool isCpuSide) {
    return getNumWordsInResp(countPerCore, subCoreOffset, isCpuSide) < totalCount;
  }

  bool isVecActive(bool isCpuSide) {
    return getPortVecCnt(isCpuSide) > 0;
  }

  // see if can do a vec load on this transition
  // otherwise need to stall. TODO don't like this that much because consumes a transition to stall?
  bool canIssueVecLoad(bool isCpuSide) {
    // // if vec load being done by another port, can't do here
    // if (isCpuSide) {
    //   if (_vecFromMem) {
    //     return false;
    //   }
    // }
    // else {
    //   if (_vecFromLLC) {
    //     return false;
    //   }
    // }

    // check if we've already issue a vec resp this cycle, and if so prevent future
    if (getPortLastVecRespTime(isCpuSide) == clockEdge()) {
      return false;
    }

    return true;
  }

  bool otherVecLoadInProgress(Addr attemptedAddr, bool isCpuSide) {
    return (attemptedAddr != getPortLastServVecAddr(isCpuSide)) &&
        (isVecActive(isCpuSide));
    // return ((_vecFromLLC && (attemptedAddr != _lastServicedVecAddr)) || _vecFromMem);
  }

  //---------------------------------------------------------------------------
  // Network ports
  //---------------------------------------------------------------------------

  // Responses to CPU side
  out_port(responseNetwork_out, LLCResponseMsg, responseFromLLC);

  // Requests coming from CPU side
  in_port(requestNetwork_in, LLCRequestMsg, requestToLLC) {
    if (requestNetwork_in.isReady(clockEdge())) {
      peek(requestNetwork_in, LLCRequestMsg) {

        //Addr LineAddress := makeLineAddress(in_msg.WordAddress);
        //assert(LineAddress == in_msg.LineAddress); why isn't this the same!!
        Addr LineAddress := in_msg.LineAddress;
        
        // sanity check
        assert(in_msg.XDim > 0 && in_msg.YDim > 0);
        
        Entry cache_entry := getCacheEntry(LineAddress);
        
        // int respCnt := getTotalResp(in_msg.CoreOffset, in_msg.SubCoreOffset, 
        //   in_msg.WordAddress, in_msg.LineAddress, in_msg.XDim, in_msg.YDim, 
        //   in_msg.CountPerCore, in_msg.PrefetchConfig);
        int respCnt := in_msg.RespCnt;
        bool isVectorReq := isVector(respCnt);

        if (isVectorReq) {
          DPRINTF(Mesh, "wakeup req addr %#x line %#x curCnt %d to cnt %d\n", 
              in_msg.WordAddress, LineAddress, getPortVecCnt(true), respCnt);
        }

        DPRINTF(RubySlicc, "%s arrived\n", in_msg);
        //DPRINTF(Mesh, "slots available %d\n", TBEs.areNSlotsAvailable(1, clockEdge()));
        if (TBEs.areNSlotsAvailable(1, clockEdge())) {
          if (is_invalid(cache_entry) &&
              cacheMemory.cacheAvail(LineAddress) == false) {

            if (isVectorReq) {
              DPRINTF(Mesh, "invalid line req addr %#x line %#x curCnt %d to cnt %d\n", 
                in_msg.WordAddress, LineAddress, getPortVecCnt(true), respCnt);
            }

            // No available cache line for this address -> trigger cache
            // replacement
            Addr victim := cacheMemory.cacheProbe(LineAddress);
            // DPRINTF(Mesh, "doing replacement on line %#x\n", LineAddress);
            trigger(Event:Repl, victim, getCacheEntry(victim), TBEs.lookup(victim));
          } else {
            
            // There's an available slot
            TBE tbe := TBEs[LineAddress];

            // can we have a stat for number of times blocked
            // and also number of times vec request was interrupted by another?
            // this make marginally better, so maybe does have to do with interupptions?
            // if (otherVecLoadInProgress(LineAddress)) {
            //   trigger(Event:VEC_STALL_WAIT, LineAddress, cache_entry, tbe);
            // }
          
            // check if vector or not (TODO should prob check RequestType rather than vlen)
            // TODO on vector, taking multiple 'transitions' to send
            // potentially blocking other things like mem req from happening for
            // a few cycles. Not really sure how faithfully any of this is modeled
            // just going to do the easiest to implement thing, which may end up
            // being worse than the baseline because blocking mem reqs more due to
            // multiple reqs.... 
            // See transitions_per_cycle in Controller.py
            if (in_msg.Type == LLCRequestType:SPLOAD ||
                  (in_msg.Type == LLCRequestType:READ && isVectorReq)) {
              // if (isVector(in_msg.RespCnt)) {
                DPRINTF(Frame, "req %d %d addr %#x line %#x curCnt %d to cntPerCore %d totCnt %d\n", 
                    canIssueVecLoad(true), getState(tbe, cache_entry, LineAddress), 
                    in_msg.WordAddress, LineAddress, getPortVecCnt(true), in_msg.CountPerCore, respCnt);
                // if currently issuing a vector load and a new one comes, then need to stallAndWait until the first one finishes
                if (otherVecLoadInProgress(LineAddress, true)) {
                  DPRINTF(Frame, "stall and wait other vec load in progess addr %#x line %#x curCnt %d to cnt %d\n", 
                    in_msg.WordAddress, LineAddress, getPortVecCnt(true), respCnt);
                  trigger(Event:VEC_STALL_WAIT, LineAddress, cache_entry, tbe);
                }
                
                if (canIssueVecLoad(true)) {
                  if (!isVectorReq || !requiresMultipleResps(respCnt, in_msg.CountPerCore, in_msg.SubCoreOffset, true)) {
                    trigger(Event:SPLOAD, LineAddress, cache_entry, tbe);
                  }
                  else if (isFirstLoadOfVec(true)) {
                    trigger(Event:VREAD0, LineAddress, cache_entry, tbe);
                  }
                  else if (!isLastLoadOfVec(respCnt, in_msg.CountPerCore, in_msg.SubCoreOffset, true)) {
                    trigger(Event:VREAD1, LineAddress, cache_entry, tbe);
                  }
                  else {
                    trigger(Event:VREAD2, LineAddress, cache_entry, tbe);
                  }
                } else {
                  DPRINTF(Frame, "stall req addr %#x line %#x curCnt %d to cnt %d\n", 
                    in_msg.WordAddress, LineAddress, getPortVecCnt(true), respCnt);
                  // DPRINTF(Mesh, "[[WARNING]] stalling req pkt %#x\n", in_msg.WordAddress);
                  // stall bro
                  trigger(Event:STALL, LineAddress, cache_entry, tbe);
                }
              // } else { // single remote store
              //   trigger(Event:SPLOAD, LineAddress, cache_entry, tbe);
              // }
            } else if (in_msg.Type == LLCRequestType:READ) {
              trigger(Event:READ, LineAddress, cache_entry, tbe);
            } else if (in_msg.Type == LLCRequestType:WRITE) {
              trigger(Event:WRITE, LineAddress, cache_entry, tbe);
            } else if (in_msg.Type == LLCRequestType:LL) {
              trigger(Event:LL, LineAddress, cache_entry, tbe);
            } else if (in_msg.Type == LLCRequestType:SC) {
              trigger(Event:SC, LineAddress, cache_entry, tbe);
            } else if (in_msg.Type == LLCRequestType:ATOMIC) {
              trigger(Event:ATOMIC, LineAddress, cache_entry, tbe);
            } else {
              error("Invalid message");
            }
          }
        } else {
          if (isVectorReq) {
            DPRINTF(Mesh, "no tbes req addr %#x line %#x curCnt %d to cnt %d\n", 
              in_msg.WordAddress, LineAddress, getPortVecCnt(true), respCnt);
          }
          TBE tbe := TBEs[LineAddress];
          trigger(Event:STALL, LineAddress, cache_entry, tbe);
        }
      }
    }
  }

  // Responses from memory controller
  in_port(memQueue_in, MemoryMsg, responseFromMemory) {
    if (memQueue_in.isReady(clockEdge())) {
      peek(memQueue_in, MemoryMsg, block_on = "addr") {
        TBE tbe := TBEs.lookup(in_msg.addr);
        assert(is_valid(tbe));

        Entry cache_entry := getCacheEntry(in_msg.addr);

        // int respCnt := getTotalResp(tbe.CoreOffset, tbe.SubCoreOffset, 
        //   tbe.WordAddress, in_msg.addr, tbe.XDim, tbe.YDim, 
        //   tbe.CountPerCore, tbe.PrefetchConfig);
        int respCnt := tbe.RespCnt;
        bool isVectorReq := isVector(respCnt);

        if (tbe.InType == LLCRequestType:SPLOAD ||
              tbe.InType == LLCRequestType:READ && isVectorReq) {

          if (otherVecLoadInProgress(in_msg.addr, false)) {
            DPRINTF(Frame, "stall and wait other vec load in progess addr %#x line %#x curCnt %d to cnt %d\n", 
                tbe.WordAddress, in_msg.addr, getPortVecCnt(false), respCnt);
            trigger(Event:VEC_STALL_WAIT_MEM, in_msg.addr, cache_entry, tbe);
          }

          // if (isVector(tbe.RespCnt)) {
            if (canIssueVecLoad(false)) {
              assert(is_valid(cache_entry));
              DPRINTF(Frame, "req %d %d addr %#x line %#x curCnt %d to cnt %d\n", 
                  canIssueVecLoad(false), getState(tbe, cache_entry, in_msg.addr), 
                  tbe.WordAddress, in_msg.addr, getPortVecCnt(false), respCnt);
              if (!isVectorReq || !requiresMultipleResps(respCnt, tbe.CountPerCore, tbe.SubCoreOffset, false)) {
                trigger(Event:SP_Mem_Data, in_msg.addr, cache_entry, tbe);
              }
              else if (isFirstLoadOfVec(false)) {
                trigger(Event:VData0, in_msg.addr, cache_entry, tbe);
              }
              else if (!isLastLoadOfVec(respCnt, tbe.CountPerCore, tbe.SubCoreOffset, false)) {
                trigger(Event:VData1, in_msg.addr, cache_entry, tbe);
              }
              else {
                trigger(Event:VData2, in_msg.addr, cache_entry, tbe);
              }
            }
            else {
               DPRINTF(Frame, "stall req addr %#x line %#x curCnt %d to cnt %d\n", 
                  tbe.WordAddress, in_msg.addr, getPortVecCnt(false), respCnt);
              // stall bro
              trigger(Event:STALL, in_msg.addr, cache_entry, tbe);
            }
          // } else {
          //   // single spread
          //   DPRINTF(Mesh, "single req %d %d\n", canIssueVecLoad(false), getState(tbe, cache_entry, in_msg.addr));
          //   trigger(Event:SP_Mem_Data, in_msg.addr, cache_entry, tbe);
          // }
        } else if (in_msg.Type == MemoryRequestType:MEMORY_READ) {
          assert(is_valid(cache_entry));
          trigger(Event:Memory_Data, in_msg.addr, cache_entry, tbe);
        } else if (in_msg.Type == MemoryRequestType:MEMORY_WB) {
          trigger(Event:Memory_Ack, in_msg.addr, cache_entry, tbe);
        } else {
          error("Invalid message");
        }
      }
    }
  }

  //---------------------------------------------------------------------------
  // Actions
  //---------------------------------------------------------------------------

  action(atb_allocateTBE, "atb", desc = "Allocate TBE") {
    TBEs.allocate(address);
    set_tbe(TBEs[address]);
  }

  action(dtb_deallocateTBE, "dtb", desc = "Deallocate TBE") {
    TBEs.deallocate(address);
    unset_tbe();
  }

  action(acb_allocateCacheBlock, "acb", desc = "Allocate cache block") {
    if (is_valid(cache_entry) == false) {
      set_cache_entry(cacheMemory.allocate(address, new Entry));
      cache_entry.LLSC_owner := createMachineID(MachineType:NULL, intToID(0));
    }
  }

  action(imr_issueMemReadRequest, "imr", desc = "Queue memory read request") {
    peek(requestNetwork_in, LLCRequestMsg) {
      queueMemoryRead(in_msg.Requestor, address, to_memory_controller_latency);
    }
  }

  action(imw_issueMemWriteBackRequest, "imw", desc = "Queue memory write request") {
    queueMemoryWrite(machineID, address, to_memory_controller_latency, cache_entry.DataBlk);
  }

  action(smu_setMRU, "smu", desc = "Set MRU") {
    assert(is_valid(cache_entry));
    cacheMemory.setMRU(cache_entry);
  }

  action(sdm_sendDataFromMem, "sdm", desc = "Send response data to CPU side from mem") {
    peek(memQueue_in, MemoryMsg) {
      int blkIdx := AddrOffset(tbe.WordAddress, in_msg.addr);
      enqueue(responseNetwork_out, LLCResponseMsg, mem_to_cpu_latency) {
        assert(is_valid(tbe));
        out_msg.Type        := LLCResponseType:DATA;
        out_msg.LineAddress := in_msg.addr;
        out_msg.Destination.add(tbe.Requestor);
        out_msg.DataBlk     := cache_entry.DataBlk;
        out_msg.MessageSize := MessageSizeType:SingleWordData;
        out_msg.SeqNum      := tbe.SeqNum;
        out_msg.BlkIdx      := blkIdx;
        out_msg.Len         := 1;
      }
    }
  }

  action(sdc_sendDataFromCache, "sdc", desc = "Send response data to CPU side from cache") {
    peek(requestNetwork_in, LLCRequestMsg) {
      int blkIdx := AddrOffset(in_msg.WordAddress, in_msg.LineAddress);
      enqueue(responseNetwork_out, LLCResponseMsg, cache_resp_latency) {
        out_msg.Type        := LLCResponseType:DATA;
        out_msg.LineAddress := in_msg.LineAddress;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.DataBlk     := cache_entry.DataBlk;
        out_msg.MessageSize := MessageSizeType:SingleWordData;
        out_msg.SeqNum      := in_msg.SeqNum;
        out_msg.BlkIdx      := blkIdx;
        out_msg.Len         := 1;
      }
    }
  }

  // these are special sends just back to the spad (not icache) where only send
  // one word of data b/c don't trace doesn't know which one to use

  action(rsc_remoteStoreFromCache, "rsc", desc = "A remote store to a spad who did not request") {
    peek(requestNetwork_in, LLCRequestMsg) {
      
      // based on vector count get the word in the cache line to send
      // this will always(?) be a linearization
      // put this in the reponse packet so know what to read
      // int linOffset := getLinearIdx(in_msg.XDim, in_msg.YDim);
      // if (!isVector(in_msg.RespCnt)) {
      //   assert(linOffset == 0);
      // }
      
      // get the spad to send this to 
      assert(machineIDToMachineType(in_msg.Requestor) == MachineType:Scratchpad);
      NodeID baseId := machineIDToNodeID(in_msg.Requestor);
      // int spadIdx := IDToInt(baseId) + linOffset;
      int coreIdx := IDToInt(baseId);
      int spadIdx := getCoreIdx(coreIdx, in_msg.CoreOffset, in_msg.SubCoreOffset, 
        in_msg.XDim, in_msg.YDim, in_msg.CountPerCore, true);
      int coreOffset := spadIdx - coreIdx;
      MachineID spad := createMachineID(MachineType:Scratchpad, intToID(spadIdx));
      
      // in case the base address is not aligned to the cache block
      // figure out the block idx for the word this reponse should send
      int wordOffset := getWordIdx(in_msg.LineAddress, in_msg.WordAddress, true);
      
      //DPRINTF(Mesh, "offset %d line %d addr %d\n", wordOffset, lineAddr, wordAddr);
      
      // if vector then send REVDATA, otherwise REDATA
      // TODO maybe should just be seperate
      Addr baseCntAddr := in_msg.PrefetchAddress;
      LLCResponseType respType;
      if (in_msg.Type == LLCRequestType:SPLOAD) {
        respType := LLCResponseType:REVDATA;
      }
      else {
        respType := LLCResponseType:REDATA;
        baseCntAddr := in_msg.WordAddress;
      }

      // because this mimics a remote store, the return address should be
      // the storage address in the spad
      Addr spadAddr := getSpadIdx(baseCntAddr, coreOffset, in_msg.SubCoreOffset, 
        in_msg.CountPerCore, true);
      
      DPRINTF(Frame, "remote store to spad %d from spad %d @ addr %#x from mem %#x lineaddr %#x offset %d\n", 
        spadIdx, coreIdx, spadAddr, in_msg.WordAddress, in_msg.LineAddress, wordOffset);
      

      
      // This functionally sends the whole packet, but the ruby network (garnet)
      // acutally looks at MessageSize to deteremine how many flits are needed
      // to serially send the data over the network.
      // TODO Shouldn't ICACHE response be larger than a singleword? 
      // and shouldn't use this same mechanism??
      enqueue(responseNetwork_out, LLCResponseMsg, cache_resp_latency) {
        out_msg.Type        := respType;
        out_msg.LineAddress := spadAddr;
        out_msg.Destination.add(spad);
        out_msg.DataBlk     := cache_entry.DataBlk;
        out_msg.MessageSize := MessageSizeType:SingleWordData;
        out_msg.SeqNum      := in_msg.SeqNum;
        // this is gem5 only meta-data, in reality would just send the single
        // word, so no need to pick out the element from the whole block
        out_msg.BlkIdx      := wordOffset;
        out_msg.Len         := getNumWordsInResp(in_msg.RespCnt, in_msg.PrefetchConfig, true); // temp encode resp size
      }
    }
  }
  

  action(rsm_remoteStoreFromMem, "rsm", desc = "A remote store to a spad who did not request from mem") {
    peek(memQueue_in, MemoryMsg) {
      
      // get the spad to send this to 
      assert(machineIDToMachineType(tbe.Requestor) == MachineType:Scratchpad);
      NodeID baseId := machineIDToNodeID(tbe.Requestor);
      int coreIdx := IDToInt(baseId);
      int spadIdx := getCoreIdx(coreIdx, tbe.CoreOffset, tbe.SubCoreOffset, 
        tbe.XDim, tbe.YDim, tbe.CountPerCore, false);
      int coreOffset := spadIdx - coreIdx;
      MachineID spad := createMachineID(MachineType:Scratchpad, intToID(spadIdx));
      
      // get offset into cache blk
      int wordOffset := getWordIdx(in_msg.addr, tbe.WordAddress, false);
      
      // if vector then send REVDATA, otherwise REDATA
      LLCResponseType respType;
      Addr baseCntAddr := tbe.PrefetchAddress;
      if (tbe.InType == LLCRequestType:SPLOAD) {
        respType := LLCResponseType:REVDATA;
      }
      else {
        respType := LLCResponseType:REDATA;
        baseCntAddr := tbe.WordAddress;
      }
    
      Addr spadAddr := getSpadIdx(baseCntAddr, coreOffset, tbe.SubCoreOffset,
        tbe.CountPerCore, false);

      DPRINTF(Frame, "remote store to spad %d from spad %d @ addr %#x from mem %#x prefetch %#x offset %d core offset %d\n", 
        spadIdx, coreIdx, spadAddr, tbe.WordAddress, tbe.PrefetchAddress, wordOffset, tbe.CoreOffset);
      
      enqueue(responseNetwork_out, LLCResponseMsg, mem_to_cpu_latency) {
        assert(is_valid(tbe));
        out_msg.Type        := respType;
        out_msg.LineAddress := spadAddr;
        out_msg.Destination.add(spad);
        out_msg.DataBlk     := cache_entry.DataBlk;
        out_msg.MessageSize := MessageSizeType:SingleWordData;
        out_msg.SeqNum      := tbe.SeqNum;
        out_msg.BlkIdx      := wordOffset;
        out_msg.Len         := getNumWordsInResp(tbe.RespCnt, tbe.PrefetchConfig, false);
      }
    }
  }
  
  // acknoweldge a prefetch was send (was a hit
  /*action(pah_prefetchAckHit, "pah", desc = "Send prefetch ack on hit") {
    peek(requestNetwork_in, LLCRequestMsg) {
      enqueue(responseNetwork_out, LLCResponseMsg, 1) {
        out_msg.Type        := LLCResponseType:ACK;
        out_msg.LineAddress := address;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Response_Control;
        out_msg.SeqNum      := in_msg.SeqNum;
      }
    }
  }
  
  action(pam_prefetchAckMiss, "pam", desc = "Send prefetch ack on miss") {
    enqueue(responseNetwork_out, LLCResponseMsg, 1) {
      assert(is_valid(tbe));
      out_msg.Type        := LLCResponseType:ACK;
      out_msg.LineAddress := address;
      out_msg.Destination.add(tbe.Requestor);
      out_msg.MessageSize := MessageSizeType:Response_Control;
      out_msg.SeqNum      := tbe.SeqNum;
    }
  }*/

  action(wdc_writeDataToCache, "wdc", desc = "Write data from mem to cache") {
    peek(memQueue_in, MemoryMsg) {
      cache_entry.DataBlk := in_msg.DataBlk;
    }
  }

  action(prq_popRequestQueue, "prq", desc = "Pop a msg from request queue") {
    requestNetwork_in.dequeue(clockEdge());
  }

  action(pmq_popMemResponseQueue, "pmq", desc = "Pop a msg from mem rsp queue") {
    memQueue_in.dequeue(clockEdge());
  }

  action(art_addRequestorToTBE, "art", desc = "Add requestor to TBE") {
    peek(requestNetwork_in, LLCRequestMsg) {
      assert(is_valid(tbe));
      tbe.Requestor := in_msg.Requestor;
      tbe.SeqNum := in_msg.SeqNum;
      tbe.WordAddress := in_msg.WordAddress;
    }
  }

  action(sdt_saveDataInTBE, "sdt", desc = "Save data from in_msg to TBE") {
    peek(requestNetwork_in, LLCRequestMsg) {
      assert(is_valid(tbe));
      tbe.DataBlk := in_msg.DataBlk;
      tbe.writeMask := in_msg.writeMask;
    }
  }
  
  action(avt_addVecInfoToTBE, "avt", desc = "Add information about vector req to TBE") {
    peek(requestNetwork_in, LLCRequestMsg) {
      assert(is_valid(tbe));
      tbe.XDim := in_msg.XDim;
      tbe.YDim := in_msg.YDim;
      // tbe.WordAddress := in_msg.WordAddress; // moved
      tbe.PrefetchAddress := in_msg.PrefetchAddress;
      tbe.CoreOffset := in_msg.CoreOffset;
      tbe.SubCoreOffset := in_msg.SubCoreOffset;
      tbe.CountPerCore := in_msg.CountPerCore;
      tbe.RespCnt := in_msg.RespCnt;
      tbe.PrefetchConfig := in_msg.PrefetchConfig;
      // tbe.Epoch := in_msg.Epoch;
      tbe.InType := in_msg.Type;
    }
  }

  action(wdd_writeDirtyDataToCache, "wdd", desc = "Write data to cache") {
    assert(is_valid(cache_entry));
    peek(requestNetwork_in, LLCRequestMsg) {
      cache_entry.DataBlk.copyPartial(in_msg.DataBlk, in_msg.writeMask);
      cache_entry.writeMask.orMask(in_msg.writeMask);
    }
  }

  action(wdt_writeDirtyDataToCacheFromTBE, "wdt", desc = "") {
    assert(is_valid(tbe));
    assert(is_valid(cache_entry));
    cache_entry.DataBlk.copyPartial(tbe.DataBlk, tbe.writeMask);
    cache_entry.writeMask.orMask(tbe.writeMask);
  }

  action(swa_sendWriteAck, "swa", desc = "Send write ack") {
    enqueue(responseNetwork_out, LLCResponseMsg, cache_resp_latency) {
      assert(is_valid(tbe));
      out_msg.Type        := LLCResponseType:ACK;
      out_msg.LineAddress := address;
      out_msg.Destination.add(tbe.Requestor);
      out_msg.MessageSize := MessageSizeType:Response_Control;
      out_msg.SeqNum      := tbe.SeqNum;
      out_msg.Len         := 1;
    }
  }

  action(dce_deallocateCacheEntry, "dce", desc = "Deallocate a cache entry") {
    if (is_valid(cache_entry)) {
      cacheMemory.deallocate(address);
    }
    unset_cache_entry();
  }

  action(sot_setLLSCOwnerFromTBE, "sot", desc = "Set LL owner") {
    assert(is_valid(tbe));
    assert(is_valid(cache_entry));
    cache_entry.LLSC_owner := tbe.Requestor;
    // DPRINTF(Mesh, "set %s as owner for %#x\n", cache_entry.LLSC_owner, address);
  }

  action(sor_setLLSCOwnerFromRequest, "sor", desc = "Set LL owner from requests") {
    assert(is_valid(cache_entry));
    peek(requestNetwork_in, LLCRequestMsg) {
      cache_entry.LLSC_owner := in_msg.Requestor;
    }
    // DPRINTF(Mesh, "set %s as owner for %#x\n", cache_entry.LLSC_owner, address);
  }

  action(scf_sendFailedSCAck, "scf", desc = "Send Failed SC ACK") {
    enqueue(responseNetwork_out, LLCResponseMsg, cache_resp_latency) {
      assert(is_valid(tbe));
      out_msg.Type        := LLCResponseType:ACK;
      out_msg.LineAddress := address;
      out_msg.Destination.add(tbe.Requestor);
      out_msg.SC_Success  := false;
      out_msg.MessageSize := MessageSizeType:Response_Control;
      out_msg.SeqNum      := tbe.SeqNum;
      out_msg.Len         := 1;
    }
  }

  action(hsc_handleSCRequest, "hsc", desc = "Handle SC request") {
    assert(is_valid(cache_entry));

    peek(requestNetwork_in, LLCRequestMsg) {
      int blkIdx := AddrOffset(in_msg.WordAddress, in_msg.LineAddress);

      if (in_msg.Requestor == cache_entry.LLSC_owner) {
        cache_entry.DataBlk.copyPartial(in_msg.DataBlk, in_msg.writeMask);
        cache_entry.writeMask.orMask(in_msg.writeMask);

        // reply requestor
        enqueue(responseNetwork_out, LLCResponseMsg, cache_resp_latency) {
          // DPRINTF(Mesh, "succeed store conditional on %#x from owner %s\n", address, in_msg.Requestor);
          out_msg.Type        := LLCResponseType:ACK;
          out_msg.LineAddress := address;
          out_msg.Destination.add(in_msg.Requestor);
          out_msg.SC_Success  := true;
          out_msg.MessageSize := MessageSizeType:Response_Control;
          out_msg.SeqNum      := in_msg.SeqNum;
          out_msg.BlkIdx      := blkIdx;
          out_msg.Len         := 1;
        }

        // set MRU
        cacheMemory.setMRU(cache_entry);
      } else {
        // send Fail ACK
        enqueue(responseNetwork_out, LLCResponseMsg, cache_resp_latency) {
          DPRINTF(Mesh, "[[WARNING]] fail store conditional on %#x from %s actual owner %s\n", address, cache_entry.LLSC_owner, in_msg.Requestor);
          // assert(is_valid(tbe)); // why???
          out_msg.Type        := LLCResponseType:ACK;
          out_msg.LineAddress := address;
          out_msg.Destination.add(in_msg.Requestor);
          out_msg.SC_Success  := false;
          out_msg.MessageSize := MessageSizeType:Response_Control;
          out_msg.SeqNum      := in_msg.SeqNum;
          out_msg.BlkIdx      := blkIdx;
          out_msg.Len         := 1;
        }
      }
    }
  }

  action(dat_doAtomicUpdate, "dat", desc = "Do atomic update") {
    assert(is_valid(tbe));
    assert(is_valid(cache_entry));

    tbe.DataBlk := cache_entry.DataBlk;
    cache_entry.DataBlk.atomicPartial(cache_entry.DataBlk, tbe.writeMask);
    cache_entry.writeMask.orMask(tbe.writeMask);

    int blkIdx := AddrOffset(tbe.WordAddress, address);

    // Make a response with old data
    enqueue(responseNetwork_out, LLCResponseMsg, cache_resp_latency) {
      out_msg.Type := LLCResponseType:DATA;
      out_msg.LineAddress := address;
      out_msg.Destination.add(tbe.Requestor);
      out_msg.DataBlk := tbe.DataBlk;
      out_msg.MessageSize := MessageSizeType:SingleWordData;
      out_msg.SeqNum := tbe.SeqNum;
      out_msg.BlkIdx := blkIdx;
      out_msg.Len    := 1;
    }
  }

  action(pht_profileHitAccess, "pht", desc = "Profile hit access") {
    ++cacheMemory.demand_hits;
  }

  action(pms_profileMissAccess, "pms", desc = "Profile miss access") {
    ++cacheMemory.demand_misses;
  }

  action(z_stall, "z", desc="stall") {
    // built-in
  }

  action(z_stallAndWait, "zsw", desc = "Stall and wait") {
    stall_and_wait(requestNetwork_in, address);
  }

  action(z_vecStallAndWait, "vsw", desc = "Stall and wait due to active vector read") {
    Addr depAddr := getPortLastServVecAddr(true);
    stall_and_wait(requestNetwork_in, depAddr);
  }

  action(z_vecStallAndWaitMem, "vsm", desc = "Stall and wait due to active vector read") {
    Addr depAddr := getPortLastServVecAddr(false);
    stall_and_wait(memQueue_in, depAddr);
  }

  action(wua_wakeupAllDependents, "wua", desc = "Wake up all buffers") {
    wakeUpAllBuffers();
  }
  
  // initialize vector request (to 0)
  action(rvm_resetVectorFromMem, "rvm", desc = "Initialize vector req counter") {
    DPRINTF(Frame, "reset vec cnt from mem\n");
    resetVecCntr(false);
    // _vecFromMem := false;
  }
  
  action(rvl_resetVectorFromLLC, "rvl", desc = "Initialize vector req counter") {
    DPRINTF(Frame, "reset vec cnt from LLC\n");
    resetVecCntr(true);
    // _vecFromLLC := false;
  }

  // mark that we've done a vector resp this cycle and don't allow anymore this cycle
  action(mvl_markVectorRespThisCycleLLC, "mvl", desc = "Mark vec resp this cycle") {
    setPortLastVecRespTime(clockEdge(), true);
    setPortLastServVecAddr(address, true);
    DPRINTF(Frame, "mark vec resp this cycle %llu addr %#x\n", 
        getPortLastVecRespTime(true), getPortLastServVecAddr(true));
  }

  action(mvm_markVectorRespThisCycleMem, "mvm", desc = "Mark vec resp this cycle") {
    setPortLastVecRespTime(clockEdge(), false);
    setPortLastServVecAddr(address, false);
    DPRINTF(Frame, "mark vec resp this cycle %llu addr %#x\n", 
        getPortLastVecRespTime(false), getPortLastServVecAddr(false));
  }
  
  // handle vector decrement
  action(ivc_incVectorCount, "ivc", desc = "Increment vector count") {
    // // enqueue the same packet back on request queue with decremented vec count
    // // if no more counts don't put back onto the queue
    peek(requestNetwork_in, LLCRequestMsg) {
    //   // might need to put in TBE when split across cache line (and you want to do other reqs)
    //   // store and then restore the count when send req and restore?
    //   // NOT neccessary because no point to do vector request across a cache line
    //   // ALSO don't need to worry about banks for this same reason b/c across a cache line
    //   increaseVecCount(in_msg.XDim);
    //   _vecFromLLC := true;
    // }
      increaseVecCount(in_msg.RespCnt, in_msg.CountPerCore, in_msg.SubCoreOffset, true);
    }
  }
  
  // for missed accesses
  action(ivt_incVectorCountTBE, "ivt", desc = "Increment vector count TBE") {
    increaseVecCount(tbe.RespCnt, tbe.CountPerCore, tbe.SubCoreOffset, false);
    // _vecFromMem := true;
  }

  // check vector error. in this case stall map interrupting in progress vector load
  // might have to give each req a counter (i.e. <10 bits from the TBE)
  action(vel_checkVectorErrorLLC, "vel", desc = "Check for known failure case during vector load") {
    if (getPortVecCnt(true) != 0) {
      DPRINTF(Frame, "[[WARNING]] preempting in flight vec req in cpu req port\n");
      assert(false);
    }
  }

  action(vem_checkVectorErrorMem, "vem", desc = "Check for known failure case during vector load") {
    if (getPortVecCnt(false) != 0) {
      DPRINTF(Frame, "[[WARNING]] preempting in flight vec req in mem resp port\n");
      assert(false);
    }
  }

  //---------------------------------------------------------------------------
  // Transitions
  //---------------------------------------------------------------------------

  // READ requests

  transition(I, { READ, SPLOAD, VREAD0 }, IV) {
    atb_allocateTBE;
    art_addRequestorToTBE;
    avt_addVecInfoToTBE;
    acb_allocateCacheBlock;
    imr_issueMemReadRequest;
    prq_popRequestQueue;
    pms_profileMissAccess;
  }

  transition(V, READ) {
    smu_setMRU;
    sdc_sendDataFromCache;
    prq_popRequestQueue;
    pht_profileHitAccess;
  }

  transition(M, READ) {
    smu_setMRU;
    sdc_sendDataFromCache;
    prq_popRequestQueue;
    pht_profileHitAccess;
  }

  transition(IV, Memory_Data, V) {
    wdc_writeDataToCache;
    smu_setMRU;
    sdm_sendDataFromMem;
    dtb_deallocateTBE;
    pmq_popMemResponseQueue;
    wua_wakeupAllDependents;
  }
  
  transition({V, M}, SPLOAD) {
    //pah_prefetchAckHit; // TODO don't need this? and acutally prob bad to wait in CPU
    smu_setMRU;
    rsc_remoteStoreFromCache;
    prq_popRequestQueue;
    pht_profileHitAccess;
  }
  
  transition(IV, SP_Mem_Data, V) {
    wdc_writeDataToCache;
    smu_setMRU;
    rsm_remoteStoreFromMem;
    dtb_deallocateTBE;
    // pms_profileMissAccess; // I think this double counts
    pmq_popMemResponseQueue;
    wua_wakeupAllDependents;
  }
  
// vector sends, go to vector, only last request is a non-vector one (like original read)
// TODO might want to have real op go first, but then the other port might mode while going
// ... both have pro-cons, but saved by serialization of message in toSPAD outport (presumably serialized)
// TODO since a prefetch is modeled as a write we place into the st queue, need to ack this
// for the master.
  transition(V, VREAD0, VV) {
    // set mru now so _very_ unlkiley to be freed while reading the line
    // eventually will want to make the line with a 'being read' to assure success
    vel_checkVectorErrorLLC;
    smu_setMRU; 
    rsc_remoteStoreFromCache;
    ivc_incVectorCount;
    // stall this so we don't handle multiple resps in the same cycle
    // and this potentially can starve other ports of transition cycles
    mvl_markVectorRespThisCycleLLC;
    //pah_prefetchAckHit;
    pht_profileHitAccess; // TODO could seperate reqs vs resps, but for now just look at resps
  }

  transition(M, VREAD0, VM) {
    vel_checkVectorErrorLLC;
    smu_setMRU; 
    rsc_remoteStoreFromCache;
    ivc_incVectorCount;
    mvl_markVectorRespThisCycleLLC;
    pht_profileHitAccess; // TODO could seperate reqs vs resps, but for now just look at resps
  }
  
  transition({VV, VM}, VREAD1) {
    rsc_remoteStoreFromCache;
    ivc_incVectorCount;
    mvl_markVectorRespThisCycleLLC;
    pht_profileHitAccess; // TODO could seperate reqs vs resps, but for now just look at resps
  }
  
  transition(VV, VREAD2, V) {
    // smu_setMRU;
    rsc_remoteStoreFromCache;
    rvl_resetVectorFromLLC; // last read (vector or not vector) so reset count
    prq_popRequestQueue;
    pht_profileHitAccess;
    wua_wakeupAllDependents;
  }

  transition(VM, VREAD2, M) {
    rsc_remoteStoreFromCache;
    rvl_resetVectorFromLLC;
    prq_popRequestQueue;
    pht_profileHitAccess;
    wua_wakeupAllDependents;
  }
  
  transition(IV, VData0) {
    vem_checkVectorErrorMem;
    wdc_writeDataToCache
    smu_setMRU;
    rsm_remoteStoreFromMem;
    ivt_incVectorCountTBE;
    mvm_markVectorRespThisCycleMem;
    pht_profileHitAccess; // count as a hit, only count the last one as a miss
    //pam_prefetchAckMiss;
  }
  
  transition(IV, VData1) {
    rsm_remoteStoreFromMem;
    ivt_incVectorCountTBE;
    mvm_markVectorRespThisCycleMem;
    pht_profileHitAccess; // count as a hit, only count the last one as a miss
  }
  
  transition(IV, VData2, V) {
    //wdc_writeDataToCache;
    // smu_setMRU;
    rsm_remoteStoreFromMem;
    rvm_resetVectorFromMem;
    dtb_deallocateTBE;
    pms_profileMissAccess; // count the final one as the miss
    pmq_popMemResponseQueue;
    wua_wakeupAllDependents;
  }

  transition({IV, MI, IM, IV_LL, IA}, { READ, SPLOAD, VREAD0, VREAD1, VREAD2 }) {
    z_stallAndWait;
  }

  // LL requests

  transition(I, LL, IV_LL) {
    atb_allocateTBE;
    art_addRequestorToTBE;
    acb_allocateCacheBlock;
    imr_issueMemReadRequest;
    prq_popRequestQueue;
    pms_profileMissAccess;
  }

  transition(V, LL) {
    smu_setMRU;
    sor_setLLSCOwnerFromRequest;
    sdc_sendDataFromCache;
    prq_popRequestQueue;
    pht_profileHitAccess;
  }

  transition(M, LL) {
    smu_setMRU;
    sor_setLLSCOwnerFromRequest;
    sdc_sendDataFromCache;
    prq_popRequestQueue;
    pht_profileHitAccess;
  }

  transition(IV_LL, Memory_Data, V) {
    wdc_writeDataToCache;
    sot_setLLSCOwnerFromTBE;
    smu_setMRU;
    sdm_sendDataFromMem;
    dtb_deallocateTBE;
    pmq_popMemResponseQueue;
    wua_wakeupAllDependents;
  }

  transition({IV, MI, IM, IV_LL, IA, VV, VM}, LL) {
    z_stallAndWait;
  }

  // WRITE requests

  transition(V, WRITE, M) {
    atb_allocateTBE;
    art_addRequestorToTBE;
    smu_setMRU;
    wdd_writeDirtyDataToCache;
    swa_sendWriteAck;
    dtb_deallocateTBE;
    prq_popRequestQueue;
    pht_profileHitAccess;
  }

  transition(M, WRITE) {
    atb_allocateTBE;
    art_addRequestorToTBE;
    smu_setMRU;
    wdd_writeDirtyDataToCache;
    swa_sendWriteAck;
    dtb_deallocateTBE;
    prq_popRequestQueue;
    pht_profileHitAccess;
  }

  transition(I, WRITE, IM) {
    atb_allocateTBE;
    art_addRequestorToTBE;
    sdt_saveDataInTBE;
    acb_allocateCacheBlock;
    imr_issueMemReadRequest;
    prq_popRequestQueue;
    pms_profileMissAccess;
  }

  transition(IM, Memory_Data, M) {
    wdc_writeDataToCache;
    wdt_writeDirtyDataToCacheFromTBE;
    swa_sendWriteAck;
    smu_setMRU;
    dtb_deallocateTBE;
    pmq_popMemResponseQueue;
    wua_wakeupAllDependents;
  }

  transition({IV, MI, IM, IV_LL, IA, VV, VM}, WRITE) {
    z_stallAndWait;
  }

  // SC requests

  transition(I, SC) {
    atb_allocateTBE;
    art_addRequestorToTBE;
    scf_sendFailedSCAck;
    dtb_deallocateTBE;
    prq_popRequestQueue;
    pms_profileMissAccess;
  }

  transition(V, SC, M) {
    hsc_handleSCRequest;
    prq_popRequestQueue;
    pht_profileHitAccess;
  }

  transition(M, SC) {
    hsc_handleSCRequest;
    prq_popRequestQueue;
    pht_profileHitAccess;
  }

  transition({IV, MI, IM, IV_LL, IA, VV, VM}, SC) {
    z_stallAndWait;
  }

  // ATOMIC requests

  transition(V, ATOMIC, M) {
    atb_allocateTBE;
    art_addRequestorToTBE;
    sdt_saveDataInTBE;
    smu_setMRU;
    dat_doAtomicUpdate;
    dtb_deallocateTBE;
    prq_popRequestQueue;
    pht_profileHitAccess;
  }

  transition(M, ATOMIC) {
    atb_allocateTBE;
    art_addRequestorToTBE;
    sdt_saveDataInTBE;
    smu_setMRU;
    dat_doAtomicUpdate;
    dtb_deallocateTBE;
    prq_popRequestQueue;
    pht_profileHitAccess;
  }

  transition(I, ATOMIC, IA) {
    atb_allocateTBE;
    art_addRequestorToTBE;
    sdt_saveDataInTBE;
    acb_allocateCacheBlock;
    imr_issueMemReadRequest;
    prq_popRequestQueue;
    pms_profileMissAccess;
  }

  transition(IA, Memory_Data, M) {
    wdc_writeDataToCache;
    dat_doAtomicUpdate;
    smu_setMRU;
    dtb_deallocateTBE;
    pmq_popMemResponseQueue;
    wua_wakeupAllDependents;
  }

  transition({IV, MI, IM, IV_LL, IA, VV, VM}, ATOMIC) {
    z_stallAndWait;
  }

  // REPL

  transition({V, I}, Repl, I) {
    dce_deallocateCacheEntry;
  }

  transition(M, Repl, MI) {
    atb_allocateTBE;
    imw_issueMemWriteBackRequest;
    dce_deallocateCacheEntry;
  }

  transition(MI, Memory_Ack, I) {
    dtb_deallocateTBE;
    pmq_popMemResponseQueue;
    wua_wakeupAllDependents;
  }

  transition({IV, MI, IM, IV_LL, IA, VV, VM}, Repl) {
    z_stallAndWait;
  }
  
  transition({V, I, M, IV, MI, IM, IV_LL, IA, VV, VM}, STALL) {
    z_stall;
  }

  transition({V, I, M, IV, MI, IM, IV_LL, IA, VV, VM}, VEC_STALL_WAIT) {
    z_vecStallAndWait;
  }

  transition({V, I, M, IV, MI, IM, IV_LL, IA, VV, VM}, VEC_STALL_WAIT_MEM) {
    z_vecStallAndWaitMem;
  }
}
