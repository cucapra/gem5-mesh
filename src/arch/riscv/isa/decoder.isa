// -*- mode:c++ -*-

// Copyright (c) 2015 RISC-V Foundation
// Copyright (c) 2017 The University of Virginia
// All rights reserved.
//
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions are
// met: redistributions of source code must retain the above copyright
// notice, this list of conditions and the following disclaimer;
// redistributions in binary form must reproduce the above copyright
// notice, this list of conditions and the following disclaimer in the
// documentation and/or other materials provided with the distribution;
// neither the name of the copyright holders nor the names of its
// contributors may be used to endorse or promote products derived from
// this software without specific prior written permission.
//
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
// "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
//
// Authors: Alec Roelke

////////////////////////////////////////////////////////////////////
//
// The RISC-V ISA decoder
//

decode QUADRANT default Unknown::unknown() {
    0x0: decode COPCODE {
        0x0: CIOp::c_addi4spn({{
            imm = CIMM8<1:1> << 2 |
                  CIMM8<0:0> << 3 |
                  CIMM8<7:6> << 4 |
                  CIMM8<5:2> << 6;
        }}, {{
            if (machInst == 0)
                fault = make_shared<IllegalInstFault>("zero instruction",
                                                      machInst);
            Rp2 = sp + imm;
        }}, uint64_t);
        format CompressedLoad {
            0x1: c_fld({{
                offset = CIMM3 << 3 | CIMM2 << 6;
            }}, {{
                Fp2_bits = Mem;
            }}, {{
                EA = Rp1 + offset;
            }});
            0x2: c_lw({{
                offset = CIMM2<1:1> << 2 |
                         CIMM3 << 3 |
                         CIMM2<0:0> << 6;
            }}, {{
                Rp2_sd = Mem_sw;
            }}, {{
                EA = Rp1 + offset;
            }});
            0x3: c_ld({{
                offset = CIMM3 << 3 | CIMM2 << 6;
            }}, {{
                Rp2_sd = Mem_sd;
            }}, {{
                EA = Rp1 + offset;
            }});
        }
        format CompressedStore {
            0x5: c_fsd({{
                offset = CIMM3 << 3 | CIMM2 << 6;
            }}, {{
                Mem = Fp2_bits;
            }}, {{
                EA = Rp1 + offset;
            }});
            0x6: c_sw({{
                offset = CIMM2<1:1> << 2 |
                         CIMM3 << 3 |
                         CIMM2<0:0> << 6;
            }}, {{
                Mem_uw = Rp2_uw;
            }}, ea_code={{
                EA = Rp1 + offset;
            }});
            0x7: c_sd({{
                offset = CIMM3 << 3 | CIMM2 << 6;
            }}, {{
                    Mem_ud = Rp2_ud;
            }}, {{
                EA = Rp1 + offset;
            }});
        }
    }
    0x1: decode COPCODE {
        format CIOp {
            0x0: c_addi({{
                imm = CIMM5;
                if (CIMM1 > 0)
                    imm |= ~((uint64_t)0x1F);
            }}, {{
                if ((RC1 == 0) != (imm == 0)) {
                    if (RC1 == 0) {
                        fault = make_shared<IllegalInstFault>("source reg x0",
                                                              machInst);
                    } else // imm == 0
                        fault = make_shared<IllegalInstFault>("immediate = 0",
                                                              machInst);
                }
                Rc1_sd = Rc1_sd + imm;
            }});
            0x1: c_addiw({{
                imm = CIMM5;
                if (CIMM1 > 0)
                    imm |= ~((uint64_t)0x1F);
            }}, {{
                if (RC1 == 0) {
                    fault = make_shared<IllegalInstFault>("source reg x0",
                                                          machInst);
                }
                Rc1_sd = (int32_t)Rc1_sd + imm;
            }});
            0x2: c_li({{
                imm = CIMM5;
                if (CIMM1 > 0)
                    imm |= ~((uint64_t)0x1F);
            }}, {{
                if (RC1 == 0) {
                    fault = make_shared<IllegalInstFault>("source reg x0",
                                                          machInst);
                }
                Rc1_sd = imm;
            }});
            0x3: decode RC1 {
                0x2: c_addi16sp({{
                    imm = CIMM5<4:4> << 4 |
                          CIMM5<0:0> << 5 |
                          CIMM5<3:3> << 6 |
                          CIMM5<2:1> << 7;
                    if (CIMM1 > 0)
                        imm |= ~((int64_t)0x1FF);
                }}, {{
                    if (imm == 0) {
                        fault = make_shared<IllegalInstFault>("immediate = 0",
                                                              machInst);
                    }
                    sp_sd = sp_sd + imm;
                }});
                default: c_lui({{
                    imm = CIMM5 << 12;
                    if (CIMM1 > 0)
                        imm |= ~((uint64_t)0x1FFFF);
                }}, {{
                    if (RC1 == 0 || RC1 == 2) {
                        fault = make_shared<IllegalInstFault>("source reg x0",
                                                              machInst);
                    }
                    if (imm == 0) {
                        fault = make_shared<IllegalInstFault>("immediate = 0",
                                                              machInst);
                    }
                    Rc1_sd = imm;
                }});
            }
        }
        0x4: decode CFUNCT2HIGH {
            format CIOp {
                0x0: c_srli({{
                    imm = CIMM5 | (CIMM1 << 5);
                }}, {{
                    if (imm == 0) {
                        fault = make_shared<IllegalInstFault>("immediate = 0",
                                                              machInst);
                    }
                    Rp1 = Rp1 >> imm;
                }}, uint64_t);
                0x1: c_srai({{
                    imm = CIMM5 | (CIMM1 << 5);
                }}, {{
                    if (imm == 0) {
                        fault = make_shared<IllegalInstFault>("immediate = 0",
                                                              machInst);
                    }
                    Rp1_sd = Rp1_sd >> imm;
                }}, uint64_t);
                0x2: c_andi({{
                    imm = CIMM5;
                    if (CIMM1 > 0)
                        imm |= ~((uint64_t)0x1F);
                }}, {{
                    Rp1 = Rp1 & imm;
                }}, uint64_t);
            }
            format ROp {
                0x3: decode CFUNCT1 {
                    0x0: decode CFUNCT2LOW {
                        0x0: c_sub({{
                            Rp1 = Rp1 - Rp2;
                        }});
                        0x1: c_xor({{
                            Rp1 = Rp1 ^ Rp2;
                        }});
                        0x2: c_or({{
                            Rp1 = Rp1 | Rp2;
                        }});
                        0x3: c_and({{
                            Rp1 = Rp1 & Rp2;
                        }});
                    }
                    0x1: decode CFUNCT2LOW {
                        0x0: c_subw({{
                            Rp1_sd = (int32_t)Rp1_sd - Rp2_sw;
                        }});
                        0x1: c_addw({{
                            Rp1_sd = (int32_t)Rp1_sd + Rp2_sw;
                        }});
                    }
                }
            }
        }
        0x5: CJOp::c_j({{
            NPC = PC + imm;
        }}, IsDirectControl, IsUncondControl);
        format CBOp {
            0x6: c_beqz({{
                if (Rp1 == 0)
                    NPC = PC + imm;
                else
                    NPC = NPC;
            }}, IsDirectControl, IsCondControl);
            0x7: c_bnez({{
                if (Rp1 != 0)
                    NPC = PC + imm;
                else
                    NPC = NPC;
            }}, IsDirectControl, IsCondControl);
        }
    }
    0x2: decode COPCODE {
        0x0: CIOp::c_slli({{
            imm = CIMM5 | (CIMM1 << 5);
        }}, {{
            if (imm == 0) {
                fault = make_shared<IllegalInstFault>("immediate = 0",
                                                      machInst);
            }
            if (RC1 == 0) {
                fault = make_shared<IllegalInstFault>("source reg x0",
                                                      machInst);
            }
            Rc1 = Rc1 << imm;
        }}, uint64_t);
        format CompressedLoad {
            0x1: c_fldsp({{
                offset = CIMM5<4:3> << 3 |
                         CIMM1 << 5 |
                         CIMM5<2:0> << 6;
            }}, {{
                Fc1_bits = Mem;
            }}, {{
                EA = sp + offset;
            }});
            0x2: c_lwsp({{
                offset = CIMM5<4:2> << 2 |
                         CIMM1 << 5 |
                         CIMM5<1:0> << 6;
            }}, {{
                if (RC1 == 0) {
                    fault = make_shared<IllegalInstFault>("source reg x0",
                                                          machInst);
                }
                Rc1_sd = Mem_sw;
            }}, {{
                EA = sp + offset;
            }});
            0x3: c_ldsp({{
                offset = CIMM5<4:3> << 3 |
                         CIMM1 << 5 |
                         CIMM5<2:0> << 6;
            }}, {{
                if (RC1 == 0) {
                    fault = make_shared<IllegalInstFault>("source reg x0",
                                                          machInst);
                }
                Rc1_sd = Mem_sd;
            }}, {{
                EA = sp + offset;
            }});
        }
        0x4: decode CFUNCT1 {
            0x0: decode RC2 {
                0x0: Jump::c_jr({{
                    if (RC1 == 0) {
                        fault = make_shared<IllegalInstFault>("source reg x0",
                                                              machInst);
                    }
                    NPC = Rc1;
                }}, IsIndirectControl, IsUncondControl, IsCall);
                default: CROp::c_mv({{
                    if (RC1 == 0) {
                        fault = make_shared<IllegalInstFault>("source reg x0",
                                                              machInst);
                    }
                    Rc1 = Rc2;
                }});
            }
            0x1: decode RC1 {
                0x0: SystemOp::c_ebreak({{
                    if (RC2 != 0) {
                        fault = make_shared<IllegalInstFault>("source reg x1",
                                                              machInst);
                    }
                    fault = make_shared<BreakpointFault>(xc->pcState());
                }}, IsSerializeAfter, IsNonSpeculative, No_OpClass);
                default: decode RC2 {
                    0x0: Jump::c_jalr({{
                        if (RC1 == 0) {
                            fault = make_shared<IllegalInstFault>
                                                        ("source reg x0",
                                                         machInst);
                        }
                        ra = NPC;
                        NPC = Rc1;
                    }}, IsIndirectControl, IsUncondControl, IsCall);
                    default: ROp::c_add({{
                        Rc1_sd = Rc1_sd + Rc2_sd;
                    }});
                }
            }
        }
        format CompressedStore {
            0x5: c_fsdsp({{
                offset = CIMM6<5:3> << 3 |
                         CIMM6<2:0> << 6;
            }}, {{
                Mem_ud = Fc2_bits;
            }}, {{
                EA = sp + offset;
            }});
            0x6: c_swsp({{
                offset = CIMM6<5:2> << 2 |
                         CIMM6<1:0> << 6;
            }}, {{
                Mem_uw = Rc2_uw;
            }}, {{
                EA = sp + offset;
            }});
            0x7: c_sdsp({{
                offset = CIMM6<5:3> << 3 |
                         CIMM6<2:0> << 6;
            }}, {{
                Mem = Rc2;
            }}, {{
                EA = sp + offset;
            }});
        }
    }
    0x3: decode OPCODE {
        0x00: decode FUNCT3 {
            format Load {
                0x0: lb({{
                    Rd_sd = Mem_sb;
                }});
                0x1: lh({{
                    Rd_sd = Mem_sh;
                }});
                0x2: lw({{
                    Rd_sd = Mem_sw;
                }});
                0x3: ld({{
                    Rd_sd = Mem_sd;
                }});
                0x4: lbu({{
                    Rd = Mem_ub;
                }});
                0x5: lhu({{
                    Rd = Mem_uh;
                }});
                0x6: lwu({{
                    Rd = Mem_uw;
                }});
                0x7: lwspec({{
                    Rd_sd = Mem_sw;
                }}, inst_flags=IsSpadSpeculative);
            }
        }

        0x01: decode FUNCT3 {
            format Load {
                0x2: flw({{
                    Fd_bits = (uint64_t)Mem_uw;
                }}, inst_flags=FloatMemReadOp);
                0x3: fld({{
                    Fd_bits = Mem;
                }}, inst_flags=FloatMemReadOp);
            }

            // OP-V VLOAD
            // 8b
            0x0: decode MOP {
                // vle8
                0x0: decode VM {
                    // vle8.v
                    0x1: VMLOp::vle8_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = VMem_sB[i];
                        }
                    }});

                    // vle8.vm
                    0x0: VMLOp::vle8_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                Vd_sd[i] = VMem_sB[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}});
                }

                // vlse8
                0x2: decode VM {
                    // vlse8.v
                    0x1: VMLSOp::vlse8_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = VMem_sB[i];
                        }
                    }}, stride=Rs2_ud);

                    // vlse8.vm
                    0x0: VMLSOp::vlse8_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                Vd_sd[i] = VMem_sB[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}}, stride=Rs2_ud);
                }

                // vlxe8
                0x3: decode VM {
                    // vlxe8.v
                    0x1: VMLXOp::vlxe8_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = VMem_sB[i];
                        }
                    }}, addr_gen={{ += Vs2_ud[i] }});

                    // vlxe8.vm
                    0x0: VMLXOp::vlxe8_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                Vd_sd[i] = VMem_sB[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}},
                        addr_gen={{ += Vs2_ud[i] }});
                }
            }

            // 16b
            0x5: decode MOP {
                // vle16
                0x0: decode VM {
                    // vle16.v
                    0x1: VMLOp::vle16_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = VMem_sH[i];
                        }
                    }});

                    // vle16.vm
                    0x0: VMLOp::vle16_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                Vd_sd[i] = VMem_sH[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}});
                }

                // vlse16
                0x2: decode VM {
                    // vlse16.v
                    0x1: VMLSOp::vlse16_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = VMem_sH[i];
                        }
                    }}, stride=Rs2_ud);

                    // vlse16.vm
                    0x0: VMLSOp::vlse16_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                Vd_sd[i] = VMem_sH[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}}, stride=Rs2_ud);
                }

                // vlxe16
                0x3: decode VM {
                    // vlxe16.v
                    0x1: VMLXOp::vlxe16_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = VMem_sH[i];
                        }
                    }}, addr_gen={{ += Vs2_ud[i] }});

                    // vlxe16.vm
                    0x0: VMLXOp::vlxe16_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                Vd_sd[i] = VMem_sH[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}},
                        addr_gen={{ += Vs2_ud[i] }});
                }
            }

            // 32b
            0x6: decode MOP {
                // vle32
                0x0: decode VM {
                    // vle32.v
                    0x1: VMLOp::vle32_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = VMem_sW[i];
                        }
                    }});

                    // vle32.vm
                    0x0: VMLOp::vle32_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                Vd_sd[i] = VMem_sW[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}});
                }

                // vlse32
                0x2: decode VM {
                    // vlse32.v
                    0x1: VMLSOp::vlse32_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = VMem_sW[i];
                        }
                    }}, stride=Rs2_ud);

                    // vlse32.vm
                    0x0: VMLSOp::vlse32_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                Vd_sd[i] = VMem_sW[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}}, stride=Rs2_ud);
                }

                // vlxe32
                0x3: decode VM {
                    // vlxe32.v
                    0x1: VMLXOp::vlxe32_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = VMem_sW[i];
                        }
                    }}, addr_gen={{ += Vs2_ud[i] }});

                    // vlxe32.vm
                    0x0: VMLXOp::vlxe32_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                Vd_sd[i] = VMem_sW[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}},
                        addr_gen={{ += Vs2_ud[i] }});
                }
            }

            // 64b
            0x7: decode MOP {
                // vle64
                0x0: decode VM {
                    // vle64.v
                    0x1: VMLOp::vle64_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = VMem_sD[i];
                        }
                    }});

                    // vle64.vm
                    0x0: VMLOp::vle64_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                Vd_sd[i] = VMem_sD[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}});
                }

                // vlse64
                0x2: decode VM {
                    // vlse64.v
                    0x1: VMLSOp::vlse64_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = VMem_sD[i];
                        }
                    }}, stride=Rs2_ud);

                    // vlse64.vm
                    0x0: VMLSOp::vlse64_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                Vd_sd[i] = VMem_sD[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}}, stride=Rs2_ud);
                }

                // vlxe64
                0x3: decode VM {
                    // vlxe64.v
                    0x1: VMLXOp::vlxe64_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = VMem_sD[i];
                        }
                    }}, addr_gen={{ += Vs2_ud[i] }});

                    // vlxe64.vm
                    0x0: VMLXOp::vlxe64_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                Vd_sd[i] = VMem_sD[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}},
                        addr_gen={{ += Vs2_ud[i] }});
                }
            }
        }

        0x03: decode FUNCT3 {
            format IOp {
                0x0: fence({{
                }}, uint64_t, IsMemBarrier, No_OpClass);
                0x1: fence_i({{
                }}, uint64_t, IsNonSpeculative, IsSerializeAfter, No_OpClass);
            }
        }

        0x04: decode FUNCT3 {
            format IOp {
                0x0: addi({{
                    Rd_sd = Rs1_sd + imm;
                }});
                0x1: slli({{
                    Rd = Rs1 << SHAMT6;
                }});
                0x2: slti({{
                    Rd = (Rs1_sd < imm) ? 1 : 0;
                }});
                0x3: sltiu({{
                    Rd = (Rs1 < imm) ? 1 : 0;
                }}, uint64_t);
                0x4: xori({{
                    Rd = Rs1 ^ imm;
                }}, uint64_t);
                0x5: decode SRTYPE {
                    0x0: srli({{
                        Rd = Rs1 >> SHAMT6;
                    }});
                    0x1: srai({{
                        Rd_sd = Rs1_sd >> SHAMT6;
                    }});
                }
                0x6: ori({{
                    Rd = Rs1 | imm;
                }}, uint64_t);
                0x7: andi({{
                    Rd = Rs1 & imm;
                }}, uint64_t);
            }
        }

        0x05: UOp::auipc({{
            Rd = PC + imm;
        }});

        0x06: decode FUNCT3 {
            format IOp {
                0x0: addiw({{
                    Rd_sd = Rs1_sw + imm;
                }}, int32_t);
                0x1: slliw({{
                    Rd_sd = Rs1_sw << SHAMT5;
                }});
                0x5: decode SRTYPE {
                    0x0: srliw({{
                        Rd_sd = (int32_t)(Rs1_uw >> SHAMT5);
                    }});
                    0x1: sraiw({{
                        Rd_sd = Rs1_sw >> SHAMT5;
                    }});
                }
                0x2: eframe ({{
                    // end frame and consume tokens
                    Rd_sd = Rs1_sd;
                }}, int32_t, IsRemem, IsNonSpeculative);
                0x3: sframe ({{
                    // begin frame and wait for tokens
                    Rd_sd = Rs1_sd;
                }}, int32_t, IsFrameStart, IsNonSpeculative);
                0x6: bcast({{
                    Rd_sd = Rs1_sd + imm;
                }}, int32_t, IsBroadcast);
                0x7: arnold({{
                    // nada, just terminator marker
                    Rd_sd = Rs1_sd;
                }}, int32_t, IsTerminator);
            }
        }

        0x08: decode FUNCT3 {
            format Store {
                0x0: sb({{
                    Mem_ub = Rs2_ub;
                }});
                0x1: sh({{
                    Mem_uh = Rs2_uh;
                }});
                0x2: sw({{
                    Mem_uw = Rs2_uw;
                }});
                0x3: sd({{
                    Mem_ud = Rs2_ud;
                }});
                //0x4: prelw({{
                //    // this op doesn't actually happen b/c execute is never
                //    // called in brg io. However, the bitwidths here are important
                //    Mem_uw = Rs2_uw;
                //}}, inst_flags=[IsSpadPrefetch, IsAckFree]);
                0x5: sw_noack({{
                    Mem_uw = Rs2_uw;
                }}, inst_flags=IsAckFree);
                0x6: plwl({{
                    Mem_uw = Rs2_uw;
                }}, inst_flags=[IsSpadPrefetch, IsLeftSide, IsAckFree]);
                0x7: plwr({{
                    Mem_uw = Rs2_uw;
                }}, inst_flags=[IsSpadPrefetch, IsAckFree]);
            }
        }

        0x09: decode FUNCT3 {
            format Store {
                0x2: fsw({{
                    Mem_uw = (uint32_t)Fs2_bits;
                }}, inst_flags=FloatMemWriteOp);
                0x3: fsd({{
                    Mem_ud = Fs2_bits;
                }}, inst_flags=FloatMemWriteOp);
            }

            // +--> OP-V VSTORE
            // 8b
            0x0: decode MOP {
                // vse8
                0x0: decode VM {
                    // vse8.v
                    0x1: VMSOp::vse8_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            VMem_uB[i] = Vs3_ud[i];
                        }
                    }});

                    // vse8.vm
                    0x0: VMSOp::vse8_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                VMem_uB[i] = Vs3_ud[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}});
                }

                // vsse8
                0x2: decode VM {
                    // vsse8.v
                    0x1: VMSSOp::vsse8_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            VMem_uB[i] = Vs3_ud[i];
                        }
                    }}, stride=Rs2_ud);

                    // vsse8.vm
                    0x0: VMSSOp::vsse8_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                VMem_uB[i] = Vs3_ud[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}}, stride=Rs2_ud);
                }

                // vsxe8
                0x3: decode VM {
                    // vsxe8.v
                    0x1: VMSXOp::vsxe8_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            VMem_uB[i] = Vs3_ud[i];
                        }
                    }}, addr_gen={{ += Vs2_ud[i] }});

                    // vsxe8.vm
                    0x0: VMSXOp::vsxe8_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                VMem_uB[i] = Vs3_ud[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}},
                        addr_gen={{ += Vs2_ud[i] }});
                }
            }

            // 16b
            0x5: decode MOP {
                // vse16
                0x0: decode VM {
                    // vse16.v
                    0x1: VMSOp::vse16_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            VMem_uH[i] = Vs3_ud[i];
                        }
                    }});

                    // vse16.vm
                    0x0: VMSOp::vse16_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                VMem_uH[i] = Vs3_ud[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}});
                }

                // vsse16
                0x2: decode VM {
                    // vsse16.v
                    0x1: VMSSOp::vsse16_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            VMem_uH[i] = Vs3_ud[i];
                        }
                    }}, stride=Rs2_ud);

                    // vsse16.vm
                    0x0: VMSSOp::vsse16_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                VMem_uH[i] = Vs3_ud[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}}, stride=Rs2_ud);
                }

                // vsxe16
                0x3: decode VM {
                    // vsxe16.v
                    0x1: VMSXOp::vsxe16_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            VMem_uH[i] = Vs3_ud[i];
                        }
                    }}, addr_gen={{ += Vs2_ud[i] }});

                    // vsxe16.vm
                    0x0: VMSXOp::vsxe16_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                VMem_uH[i] = Vs3_ud[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}},
                        addr_gen={{ += Vs2_ud[i] }});
                }
            }

            // 32b
            0x6: decode MOP {
                // vse32
                0x0: decode VM {
                    // vse32.v
                    0x1: VMSOp::vse32_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            VMem_uW[i] = Vs3_ud[i];
                        }
                    }});

                    // vse32.vm
                    0x0: VMSOp::vse32_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                VMem_uW[i] = Vs3_ud[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}});
                }

                // vsse32
                0x2: decode VM {
                    // vsse32.v
                    0x1: VMSSOp::vsse32_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            VMem_uW[i] = Vs3_ud[i];
                        }
                    }}, stride=Rs2_ud);

                    // vsse32.vm
                    0x0: VMSSOp::vsse32_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                VMem_uW[i] = Vs3_ud[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}}, stride=Rs2_ud);
                }

                // vsxe32
                0x3: decode VM {
                    // vsxe32.v
                    0x1: VMSXOp::vsxe32_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            VMem_uW[i] = Vs3_ud[i];
                        }
                    }}, addr_gen={{ += Vs2_ud[i] }});

                    // vsxe32.vm
                    0x0: VMSXOp::vsxe32_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                VMem_uW[i] = Vs3_ud[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}},
                        addr_gen={{ += Vs2_ud[i] }});
                }
            }

            // 64b
            0x7: decode MOP {
                // vse64
                0x0: decode VM {
                    // vse64.v
                    0x1: VMSOp::vse64_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            VMem_uD[i] = Vs3_ud[i];
                        }
                    }});

                    // vse64.vm
                    0x0: VMSOp::vse64_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                VMem_uD[i] = Vs3_ud[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}});
                }

                // vsse64
                0x2: decode VM {
                    // vsse64.v
                    0x1: VMSSOp::vsse64_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            VMem_uD[i] = Vs3_ud[i];
                        }
                    }}, stride=Rs2_ud);

                    // vsse64.vm
                    0x0: VMSSOp::vsse64_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                VMem_uD[i] = Vs3_ud[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}}, stride=Rs2_ud);
                }

                // vsxe64
                0x3: decode VM {
                    // vsxe64.v
                    0x1: VMSXOp::vsxe64_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            VMem_uD[i] = Vs3_ud[i];
                        }
                    }}, addr_gen={{ += Vs2_ud[i] }});

                    // vsxe64.vm
                    0x0: VMSXOp::vsxe64_vm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1) {
                                VMem_uD[i] = Vs3_ud[i];
                            }
                        }
                    }}, mask={{Vm_ud[i] & 0x1}},
                        addr_gen={{ += Vs2_ud[i] }});
                }
            }
        }

        0x0b: decode FUNCT3 {
            0x2: decode AMOFUNCT {
                0x2: LoadReserved::lr_w({{
                    Rd_sd = Mem_sw;
                }}, mem_flags=LLSC);
                0x3: StoreCond::sc_w({{
                    Mem_uw = Rs2_uw;
                }}, {{
                    Rd = result;
                }}, inst_flags=IsStoreConditional, mem_flags=LLSC);
                0x0: AtomicMemOp::amoadd_w({{
                    Rd_sd = Mem_sw;
                }}, {{
                    TypedAtomicOpFunctor<int32_t> *amo_op =
                          new AtomicGenericOp<int32_t>(Rs2_sw,
                                  [](int32_t* b, int32_t a){ *b += a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x1: AtomicMemOp::amoswap_w({{
                    Rd_sd = Mem_sw;
                }}, {{
                    TypedAtomicOpFunctor<uint32_t> *amo_op =
                          new AtomicGenericOp<uint32_t>(Rs2_uw,
                                  [](uint32_t* b, uint32_t a){ *b = a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x4: AtomicMemOp::amoxor_w({{
                    Rd_sd = Mem_sw;
                }}, {{
                    TypedAtomicOpFunctor<uint32_t> *amo_op =
                          new AtomicGenericOp<uint32_t>(Rs2_uw,
                                  [](uint32_t* b, uint32_t a){ *b ^= a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x8: AtomicMemOp::amoor_w({{
                    Rd_sd = Mem_sw;
                }}, {{
                    TypedAtomicOpFunctor<uint32_t> *amo_op =
                          new AtomicGenericOp<uint32_t>(Rs2_uw,
                                  [](uint32_t* b, uint32_t a){ *b |= a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0xc: AtomicMemOp::amoand_w({{
                    Rd_sd = Mem_sw;
                }}, {{
                    TypedAtomicOpFunctor<uint32_t> *amo_op =
                          new AtomicGenericOp<uint32_t>(Rs2_uw,
                                  [](uint32_t* b, uint32_t a){ *b &= a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x10: AtomicMemOp::amomin_w({{
                    Rd_sd = Mem_sw;
                }}, {{
                    TypedAtomicOpFunctor<int32_t> *amo_op =
                      new AtomicGenericOp<int32_t>(Rs2_sw,
                        [](int32_t* b, int32_t a){ if (a < *b) *b = a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x14: AtomicMemOp::amomax_w({{
                    Rd_sd = Mem_sw;
                }}, {{
                    TypedAtomicOpFunctor<int32_t> *amo_op =
                      new AtomicGenericOp<int32_t>(Rs2_sw,
                        [](int32_t* b, int32_t a){ if (a > *b) *b = a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x18: AtomicMemOp::amominu_w({{
                    Rd_sd = Mem_sw;
                }}, {{
                    TypedAtomicOpFunctor<uint32_t> *amo_op =
                      new AtomicGenericOp<uint32_t>(Rs2_uw,
                        [](uint32_t* b, uint32_t a){ if (a < *b) *b = a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x1c: AtomicMemOp::amomaxu_w({{
                    Rd_sd = Mem_sw;
                }}, {{
                    TypedAtomicOpFunctor<uint32_t> *amo_op =
                      new AtomicGenericOp<uint32_t>(Rs2_uw,
                        [](uint32_t* b, uint32_t a){ if (a > *b) *b = a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
            }
            0x3: decode AMOFUNCT {
                0x2: LoadReserved::lr_d({{
                    Rd_sd = Mem_sd;
                }}, mem_flags=LLSC);
                0x3: StoreCond::sc_d({{
                    Mem = Rs2;
                }}, {{
                    Rd = result;
                }}, mem_flags=LLSC, inst_flags=IsStoreConditional);
                0x0: AtomicMemOp::amoadd_d({{
                    Rd_sd = Mem_sd;
                }}, {{
                    TypedAtomicOpFunctor<int64_t> *amo_op =
                          new AtomicGenericOp<int64_t>(Rs2_sd,
                                  [](int64_t* b, int64_t a){ *b += a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x1: AtomicMemOp::amoswap_d({{
                    Rd_sd = Mem_sd;
                }}, {{
                    TypedAtomicOpFunctor<uint64_t> *amo_op =
                          new AtomicGenericOp<uint64_t>(Rs2_ud,
                                  [](uint64_t* b, uint64_t a){ *b = a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x4: AtomicMemOp::amoxor_d({{
                    Rd_sd = Mem_sd;
                }}, {{
                    TypedAtomicOpFunctor<uint64_t> *amo_op =
                          new AtomicGenericOp<uint64_t>(Rs2_ud,
                                 [](uint64_t* b, uint64_t a){ *b ^= a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x8: AtomicMemOp::amoor_d({{
                    Rd_sd = Mem_sd;
                }}, {{
                    TypedAtomicOpFunctor<uint64_t> *amo_op =
                          new AtomicGenericOp<uint64_t>(Rs2_ud,
                                 [](uint64_t* b, uint64_t a){ *b |= a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0xc: AtomicMemOp::amoand_d({{
                    Rd_sd = Mem_sd;
                }}, {{
                    TypedAtomicOpFunctor<uint64_t> *amo_op =
                          new AtomicGenericOp<uint64_t>(Rs2_ud,
                                 [](uint64_t* b, uint64_t a){ *b &= a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x10: AtomicMemOp::amomin_d({{
                    Rd_sd = Mem_sd;
                }}, {{
                    TypedAtomicOpFunctor<int64_t> *amo_op =
                      new AtomicGenericOp<int64_t>(Rs2_sd,
                        [](int64_t* b, int64_t a){ if (a < *b) *b = a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x14: AtomicMemOp::amomax_d({{
                    Rd_sd = Mem_sd;
                }}, {{
                    TypedAtomicOpFunctor<int64_t> *amo_op =
                      new AtomicGenericOp<int64_t>(Rs2_sd,
                        [](int64_t* b, int64_t a){ if (a > *b) *b = a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x18: AtomicMemOp::amominu_d({{
                    Rd_sd = Mem_sd;
                }}, {{
                    TypedAtomicOpFunctor<uint64_t> *amo_op =
                      new AtomicGenericOp<uint64_t>(Rs2_ud,
                        [](uint64_t* b, uint64_t a){ if (a < *b) *b = a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x1c: AtomicMemOp::amomaxu_d({{
                    Rd_sd = Mem_sd;
                }}, {{
                    TypedAtomicOpFunctor<uint64_t> *amo_op =
                      new AtomicGenericOp<uint64_t>(Rs2_ud,
                        [](uint64_t* b, uint64_t a){ if (a > *b) *b = a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
            }
        }
        0x0c: decode FUNCT3 {
            format ROp {
                0x0: decode FUNCT7 {
                    0x0: add({{
                        Rd = Rs1_sd + Rs2_sd;
                    }});
                    0x1: mul({{
                        Rd = Rs1_sd*Rs2_sd;
                    }}, IntMultOp);
                    0x20: sub({{
                        Rd = Rs1_sd - Rs2_sd;
                    }});
                }
                0x1: decode FUNCT7 {
                    0x0: sll({{
                        Rd = Rs1 << Rs2<5:0>;
                    }});
                    0x1: mulh({{
                        bool negate = (Rs1_sd < 0) != (Rs2_sd < 0);

                        uint64_t Rs1_lo = (uint32_t)abs(Rs1_sd);
                        uint64_t Rs1_hi = (uint64_t)abs(Rs1_sd) >> 32;
                        uint64_t Rs2_lo = (uint32_t)abs(Rs2_sd);
                        uint64_t Rs2_hi = (uint64_t)abs(Rs2_sd) >> 32;

                        uint64_t hi = Rs1_hi*Rs2_hi;
                        uint64_t mid1 = Rs1_hi*Rs2_lo;
                        uint64_t mid2 = Rs1_lo*Rs2_hi;
                        uint64_t lo = Rs2_lo*Rs1_lo;
                        uint64_t carry = ((uint64_t)(uint32_t)mid1
                                + (uint64_t)(uint32_t)mid2 + (lo >> 32)) >> 32;

                        uint64_t res = hi +
                                       (mid1 >> 32) +
                                       (mid2 >> 32) +
                                       carry;
                        Rd = negate ? ~res + (Rs1_sd*Rs2_sd == 0 ? 1 : 0)
                                    : res;
                    }}, IntMultOp);
                }
                0x2: decode FUNCT7 {
                    0x0: slt({{
                        Rd = (Rs1_sd < Rs2_sd) ? 1 : 0;
                    }});
                    0x1: mulhsu({{
                        bool negate = Rs1_sd < 0;
                        uint64_t Rs1_lo = (uint32_t)abs(Rs1_sd);
                        uint64_t Rs1_hi = (uint64_t)abs(Rs1_sd) >> 32;
                        uint64_t Rs2_lo = (uint32_t)Rs2;
                        uint64_t Rs2_hi = Rs2 >> 32;

                        uint64_t hi = Rs1_hi*Rs2_hi;
                        uint64_t mid1 = Rs1_hi*Rs2_lo;
                        uint64_t mid2 = Rs1_lo*Rs2_hi;
                        uint64_t lo = Rs1_lo*Rs2_lo;
                        uint64_t carry = ((uint64_t)(uint32_t)mid1
                                + (uint64_t)(uint32_t)mid2 + (lo >> 32)) >> 32;

                        uint64_t res = hi +
                                       (mid1 >> 32) +
                                       (mid2 >> 32) +
                                       carry;
                        Rd = negate ? ~res + (Rs1_sd*Rs2 == 0 ? 1 : 0) : res;
                    }}, IntMultOp);
                }
                0x3: decode FUNCT7 {
                    0x0: sltu({{
                        Rd = (Rs1 < Rs2) ? 1 : 0;
                    }});
                    0x1: mulhu({{
                        uint64_t Rs1_lo = (uint32_t)Rs1;
                        uint64_t Rs1_hi = Rs1 >> 32;
                        uint64_t Rs2_lo = (uint32_t)Rs2;
                        uint64_t Rs2_hi = Rs2 >> 32;

                        uint64_t hi = Rs1_hi*Rs2_hi;
                        uint64_t mid1 = Rs1_hi*Rs2_lo;
                        uint64_t mid2 = Rs1_lo*Rs2_hi;
                        uint64_t lo = Rs1_lo*Rs2_lo;
                        uint64_t carry = ((uint64_t)(uint32_t)mid1
                                + (uint64_t)(uint32_t)mid2 + (lo >> 32)) >> 32;

                        Rd = hi + (mid1 >> 32) + (mid2 >> 32) + carry;
                    }}, IntMultOp);
                }
                0x4: decode FUNCT7 {
                    0x0: xor({{
                        Rd = Rs1 ^ Rs2;
                    }});
                    0x1: div({{
                        if (Rs2_sd == 0) {
                            Rd_sd = -1;
                        } else if (Rs1_sd == numeric_limits<int64_t>::min()
                                && Rs2_sd == -1) {
                            Rd_sd = numeric_limits<int64_t>::min();
                        } else {
                            Rd_sd = Rs1_sd/Rs2_sd;
                        }
                    }}, IntDivOp);
                }
                0x5: decode FUNCT7 {
                    0x0: srl({{
                        Rd = Rs1 >> Rs2<5:0>;
                    }});
                    0x1: divu({{
                        if (Rs2 == 0) {
                            Rd = numeric_limits<uint64_t>::max();
                        } else {
                            Rd = Rs1/Rs2;
                        }
                    }}, IntDivOp);
                    0x20: sra({{
                        Rd_sd = Rs1_sd >> Rs2<5:0>;
                    }});
                }
                0x6: decode FUNCT7 {
                    0x0: or({{
                        Rd = Rs1 | Rs2;
                    }});
                    0x1: rem({{
                        if (Rs2_sd == 0) {
                            Rd = Rs1_sd;
                        } else if (Rs1_sd == numeric_limits<int64_t>::min()
                                && Rs2_sd == -1) {
                            Rd = 0;
                        } else {
                            Rd = Rs1_sd%Rs2_sd;
                        }
                    }}, IntDivOp);
                }
                0x7: decode FUNCT7 {
                    0x0: and({{
                        Rd = Rs1 & Rs2;
                    }});
                    0x1: remu({{
                        if (Rs2 == 0) {
                            Rd = Rs1;
                        } else {
                            Rd = Rs1%Rs2;
                        }
                    }}, IntDivOp);

                    // rd should be x0 b/c using special register here
                    // sets a single predicate flags that all instructions
                    // will use to determine whether they should execute or not (in vector mode)
                    0x5: cmpeq({{
                        Rd = Rs1 == Rs2;
                    }}, IsPredicate, IsPredEq);
                    0x6: cmpneq({{
                        Rd = Rs1 != Rs2;
                    }}, IsPredicate, IsPredNeq);
                }
            }
        }

        0x0d: UOp::lui({{
            Rd = (uint64_t)imm;
        }});

        0x0e: decode FUNCT3 {
            format ROp {
                0x0: decode FUNCT7 {
                    0x0: addw({{
                        Rd_sd = Rs1_sw + Rs2_sw;
                    }});
                    0x1: mulw({{
                        Rd_sd = (int32_t)(Rs1_sw*Rs2_sw);
                    }}, IntMultOp);
                    0x20: subw({{
                        Rd_sd = Rs1_sw - Rs2_sw;
                    }});
                }
                0x1: sllw({{
                    Rd_sd = Rs1_sw << Rs2<4:0>;
                }});
                0x4: divw({{
                    if (Rs2_sw == 0) {
                        Rd_sd = -1;
                    } else if (Rs1_sw == numeric_limits<int32_t>::min()
                            && Rs2_sw == -1) {
                        Rd_sd = numeric_limits<int32_t>::min();
                    } else {
                        Rd_sd = Rs1_sw/Rs2_sw;
                    }
                }}, IntDivOp);
                0x5: decode FUNCT7 {
                    0x0: srlw({{
                        Rd_sd = (int32_t)(Rs1_uw >> Rs2<4:0>);
                    }});
                    0x1: divuw({{
                        if (Rs2_uw == 0) {
                            Rd_sd = numeric_limits<uint64_t>::max();
                        } else {
                            Rd_sd = (int32_t)(Rs1_uw/Rs2_uw);
                        }
                    }}, IntDivOp);
                    0x20: sraw({{
                        Rd_sd = Rs1_sw >> Rs2<4:0>;
                    }});
                }
                0x6: remw({{
                    if (Rs2_sw == 0) {
                        Rd_sd = Rs1_sw;
                    } else if (Rs1_sw == numeric_limits<int32_t>::min()
                            && Rs2_sw == -1) {
                        Rd_sd = 0;
                    } else {
                        Rd_sd = Rs1_sw%Rs2_sw;
                    }
                }}, IntDivOp);
                0x7: remuw({{
                    if (Rs2_uw == 0) {
                        Rd_sd = (int32_t)Rs1_uw;
                    } else {
                        Rd_sd = (int32_t)(Rs1_uw%Rs2_uw);
                    }
                }}, IntDivOp);
            }
        }

        format FPROp {
            0x10: decode FUNCT2 {
                0x0: fmadd_s({{
                    uint32_t temp;
                    float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                    float fs2 = reinterpret_cast<float&>(temp = Fs2_bits);
                    float fs3 = reinterpret_cast<float&>(temp = Fs3_bits);
                    float fd;

                    if (std::isnan(fs1) || std::isnan(fs2) ||
                            std::isnan(fs3)) {
                        if (issignalingnan(fs1) || issignalingnan(fs2)
                                || issignalingnan(fs3)) {
                            FFLAGS |= FloatInvalid;
                        }
                        fd = numeric_limits<float>::quiet_NaN();
                    } else if (std::isinf(fs1) || std::isinf(fs2) ||
                            std::isinf(fs3)) {
                        if (signbit(fs1) == signbit(fs2)
                                && !std::isinf(fs3)) {
                            fd = numeric_limits<float>::infinity();
                        } else if (signbit(fs1) != signbit(fs2)
                                && !std::isinf(fs3)) {
                            fd = -numeric_limits<float>::infinity();
                        } else { // Fs3_sf is infinity
                            fd = fs3;
                        }
                    } else {
                        fd = fs1*fs2 + fs3;
                    }
                    Fd_bits = (uint64_t)reinterpret_cast<uint32_t&>(fd);
                }}, FloatMultAccOp);
                0x1: fmadd_d({{
                    if (std::isnan(Fs1) || std::isnan(Fs2) ||
                            std::isnan(Fs3)) {
                        if (issignalingnan(Fs1) || issignalingnan(Fs2)
                                || issignalingnan(Fs3)) {
                            FFLAGS |= FloatInvalid;
                        }
                        Fd = numeric_limits<double>::quiet_NaN();
                    } else if (std::isinf(Fs1) || std::isinf(Fs2) ||
                            std::isinf(Fs3)) {
                        if (signbit(Fs1) == signbit(Fs2)
                                && !std::isinf(Fs3)) {
                            Fd = numeric_limits<double>::infinity();
                        } else if (signbit(Fs1) != signbit(Fs2)
                                && !std::isinf(Fs3)) {
                            Fd = -numeric_limits<double>::infinity();
                        } else {
                            Fd = Fs3;
                        }
                    } else {
                        Fd = Fs1*Fs2 + Fs3;
                    }
                }}, FloatMultAccOp);
            }
            0x11: decode FUNCT2 {
                0x0: fmsub_s({{
                    uint32_t temp;
                    float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                    float fs2 = reinterpret_cast<float&>(temp = Fs2_bits);
                    float fs3 = reinterpret_cast<float&>(temp = Fs3_bits);
                    float fd;

                    if (std::isnan(fs1) || std::isnan(fs2) ||
                            std::isnan(fs3)) {
                        if (issignalingnan(fs1) || issignalingnan(fs2)
                                || issignalingnan(fs3)) {
                            FFLAGS |= FloatInvalid;
                        }
                        fd = numeric_limits<float>::quiet_NaN();
                    } else if (std::isinf(fs1) || std::isinf(fs2) ||
                            std::isinf(fs3)) {
                        if (signbit(fs1) == signbit(fs2)
                                && !std::isinf(fs3)) {
                            fd = numeric_limits<float>::infinity();
                        } else if (signbit(fs1) != signbit(fs2)
                                && !std::isinf(fs3)) {
                            fd = -numeric_limits<float>::infinity();
                        } else { // Fs3_sf is infinity
                            fd = -fs3;
                        }
                    } else {
                        fd = fs1*fs2 - fs3;
                    }
                    Fd_bits = (uint64_t)reinterpret_cast<uint32_t&>(fd);
                }}, FloatMultAccOp);
                0x1: fmsub_d({{
                    if (std::isnan(Fs1) || std::isnan(Fs2) ||
                            std::isnan(Fs3)) {
                        if (issignalingnan(Fs1) || issignalingnan(Fs2)
                                || issignalingnan(Fs3)) {
                            FFLAGS |= FloatInvalid;
                        }
                        Fd = numeric_limits<double>::quiet_NaN();
                    } else if (std::isinf(Fs1) || std::isinf(Fs2) ||
                            std::isinf(Fs3)) {
                        if (signbit(Fs1) == signbit(Fs2)
                                && !std::isinf(Fs3)) {
                            Fd = numeric_limits<double>::infinity();
                        } else if (signbit(Fs1) != signbit(Fs2)
                                && !std::isinf(Fs3)) {
                            Fd = -numeric_limits<double>::infinity();
                        } else {
                            Fd = -Fs3;
                        }
                    } else {
                        Fd = Fs1*Fs2 - Fs3;
                    }
                }}, FloatMultAccOp);
            }
            0x12: decode FUNCT2 {
                0x0: fnmsub_s({{
                    uint32_t temp;
                    float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                    float fs2 = reinterpret_cast<float&>(temp = Fs2_bits);
                    float fs3 = reinterpret_cast<float&>(temp = Fs3_bits);
                    float fd;

                    if (std::isnan(fs1) || std::isnan(fs2) ||
                            std::isnan(fs3)) {
                        if (issignalingnan(fs1) || issignalingnan(fs2)
                                || issignalingnan(fs3)) {
                            FFLAGS |= FloatInvalid;
                        }
                        fd = numeric_limits<float>::quiet_NaN();
                    } else if (std::isinf(fs1) || std::isinf(fs2) ||
                            std::isinf(fs3)) {
                        if (signbit(fs1) == signbit(fs2)
                                && !std::isinf(fs3)) {
                            fd = -numeric_limits<float>::infinity();
                        } else if (signbit(fs1) != signbit(fs2)
                                && !std::isinf(fs3)) {
                            fd = numeric_limits<float>::infinity();
                        } else { // Fs3_sf is infinity
                            fd = fs3;
                        }
                    } else {
                        fd = -(fs1*fs2 - fs3);
                    }
                    Fd_bits = (uint64_t)reinterpret_cast<uint32_t&>(fd);
                }}, FloatMultAccOp);
                0x1: fnmsub_d({{
                    if (std::isnan(Fs1) || std::isnan(Fs2) ||
                            std::isnan(Fs3)) {
                        if (issignalingnan(Fs1) || issignalingnan(Fs2)
                                || issignalingnan(Fs3)) {
                            FFLAGS |= FloatInvalid;
                        }
                        Fd = numeric_limits<double>::quiet_NaN();
                    } else if (std::isinf(Fs1) || std::isinf(Fs2)
                            || std::isinf(Fs3)) {
                        if (signbit(Fs1) == signbit(Fs2)
                                && !std::isinf(Fs3)) {
                            Fd = -numeric_limits<double>::infinity();
                        } else if (signbit(Fs1) != signbit(Fs2)
                                && !std::isinf(Fs3)) {
                            Fd = numeric_limits<double>::infinity();
                        } else {
                            Fd = Fs3;
                        }
                    } else {
                        Fd = -(Fs1*Fs2 - Fs3);
                    }
                }}, FloatMultAccOp);
            }
            0x13: decode FUNCT2 {
                0x0: fnmadd_s({{
                    uint32_t temp;
                    float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                    float fs2 = reinterpret_cast<float&>(temp = Fs2_bits);
                    float fs3 = reinterpret_cast<float&>(temp = Fs3_bits);
                    float fd;

                    if (std::isnan(fs1) || std::isnan(fs2) ||
                            std::isnan(fs3)) {
                        if (issignalingnan(fs1) || issignalingnan(fs2)
                                || issignalingnan(fs3)) {
                            FFLAGS |= FloatInvalid;
                        }
                        fd = numeric_limits<float>::quiet_NaN();
                    } else if (std::isinf(fs1) || std::isinf(fs2) ||
                            std::isinf(fs3)) {
                        if (signbit(fs1) == signbit(fs2)
                                && !std::isinf(fs3)) {
                            fd = -numeric_limits<float>::infinity();
                        } else if (signbit(fs1) != signbit(fs2)
                                && !std::isinf(fs3)) {
                            fd = numeric_limits<float>::infinity();
                        } else { // Fs3_sf is infinity
                            fd = -fs3;
                        }
                    } else {
                        fd = -(fs1*fs2 + fs3);
                    }
                    Fd_bits = (uint64_t)reinterpret_cast<uint32_t&>(fd);
                }}, FloatMultAccOp);
                0x1: fnmadd_d({{
                    if (std::isnan(Fs1) || std::isnan(Fs2) ||
                            std::isnan(Fs3)) {
                        if (issignalingnan(Fs1) || issignalingnan(Fs2)
                                || issignalingnan(Fs3)) {
                            FFLAGS |= FloatInvalid;
                        }
                        Fd = numeric_limits<double>::quiet_NaN();
                    } else if (std::isinf(Fs1) || std::isinf(Fs2) ||
                            std::isinf(Fs3)) {
                        if (signbit(Fs1) == signbit(Fs2)
                                && !std::isinf(Fs3)) {
                            Fd = -numeric_limits<double>::infinity();
                        } else if (signbit(Fs1) != signbit(Fs2)
                                && !std::isinf(Fs3)) {
                            Fd = numeric_limits<double>::infinity();
                        } else {
                            Fd = -Fs3;
                        }
                    } else {
                        Fd = -(Fs1*Fs2 + Fs3);
                    }
                }}, FloatMultAccOp);
            }
            0x14: decode FUNCT7 {
                0x0: fadd_s({{
                    uint32_t temp;
                    float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                    float fs2 = reinterpret_cast<float&>(temp = Fs2_bits);
                    float fd;

                    if (std::isnan(fs1) || std::isnan(fs2)) {
                        if (issignalingnan(fs1) || issignalingnan(fs2)) {
                            FFLAGS |= FloatInvalid;
                        }
                        fd = numeric_limits<float>::quiet_NaN();
                    } else {
                        fd = fs1 + fs2;
                    }
                    Fd_bits = (uint64_t)reinterpret_cast<uint32_t&>(fd);
                }}, FloatAddOp);
                0x1: fadd_d({{
                    if (std::isnan(Fs1) || std::isnan(Fs2)) {
                        if (issignalingnan(Fs1) || issignalingnan(Fs2)) {
                            FFLAGS |= FloatInvalid;
                        }
                        Fd = numeric_limits<double>::quiet_NaN();
                    } else {
                        Fd = Fs1 + Fs2;
                    }
                }}, FloatAddOp);
                0x4: fsub_s({{
                    uint32_t temp;
                    float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                    float fs2 = reinterpret_cast<float&>(temp = Fs2_bits);
                    float fd;

                    if (std::isnan(fs1) || std::isnan(fs2)) {
                        if (issignalingnan(fs1) || issignalingnan(fs2)) {
                            FFLAGS |= FloatInvalid;
                        }
                        fd = numeric_limits<float>::quiet_NaN();
                    } else {
                        fd = fs1 - fs2;
                    }
                    Fd_bits = (uint64_t)reinterpret_cast<uint32_t&>(fd);
                }}, FloatAddOp);
                0x5: fsub_d({{
                    if (std::isnan(Fs1) || std::isnan(Fs2)) {
                        if (issignalingnan(Fs1) || issignalingnan(Fs2)) {
                            FFLAGS |= FloatInvalid;
                        }
                        Fd = numeric_limits<double>::quiet_NaN();
                    } else {
                        Fd = Fs1 - Fs2;
                    }
                }}, FloatAddOp);
                0x8: fmul_s({{
                    uint32_t temp;
                    float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                    float fs2 = reinterpret_cast<float&>(temp = Fs2_bits);
                    float fd;

                    if (std::isnan(fs1) || std::isnan(fs2)) {
                        if (issignalingnan(fs1) || issignalingnan(fs2)) {
                            FFLAGS |= FloatInvalid;
                        }
                        fd = numeric_limits<float>::quiet_NaN();
                    } else {
                        fd = fs1*fs2;
                    }
                    Fd_bits = (uint64_t)reinterpret_cast<uint32_t&>(fd);
                }}, FloatMultOp);
                0x9: fmul_d({{
                    if (std::isnan(Fs1) || std::isnan(Fs2)) {
                        if (issignalingnan(Fs1) || issignalingnan(Fs2)) {
                            FFLAGS |= FloatInvalid;
                        }
                        Fd = numeric_limits<double>::quiet_NaN();
                    } else {
                        Fd = Fs1*Fs2;
                    }
                }}, FloatMultOp);
                0xc: fdiv_s({{
                    uint32_t temp;
                    float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                    float fs2 = reinterpret_cast<float&>(temp = Fs2_bits);
                    float fd;

                    if (std::isnan(fs1) || std::isnan(fs2)) {
                        if (issignalingnan(fs1) || issignalingnan(fs2)) {
                            FFLAGS |= FloatInvalid;
                        }
                        fd = numeric_limits<float>::quiet_NaN();
                    } else {
                        fd = fs1/fs2;
                    }
                    Fd_bits = (uint64_t)reinterpret_cast<uint32_t&>(fd);
                }}, FloatDivOp);
                0xd: fdiv_d({{
                    if (std::isnan(Fs1) || std::isnan(Fs2)) {
                        if (issignalingnan(Fs1) || issignalingnan(Fs2)) {
                            FFLAGS |= FloatInvalid;
                        }
                        Fd = numeric_limits<double>::quiet_NaN();
                    } else {
                        Fd = Fs1/Fs2;
                    }
                }}, FloatDivOp);
                0x10: decode ROUND_MODE {
                    0x0: fsgnj_s({{
                        uint32_t temp;
                        float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                        float fs2 = reinterpret_cast<float&>(temp = Fs2_bits);
                        float fd;

                        if (issignalingnan(fs1)) {
                            fd = numeric_limits<float>::signaling_NaN();
                            feclearexcept(FE_INVALID);
                        } else {
                            fd = copysign(fs1, fs2);
                        }
                        Fd_bits = (uint64_t)reinterpret_cast<uint32_t&>(fd);
                    }}, FloatMiscOp);
                    0x1: fsgnjn_s({{
                        uint32_t temp;
                        float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                        float fs2 = reinterpret_cast<float&>(temp = Fs2_bits);
                        float fd;

                        if (issignalingnan(fs1)) {
                            fd = numeric_limits<float>::signaling_NaN();
                            feclearexcept(FE_INVALID);
                        } else {
                            fd = copysign(fs1, -fs2);
                        }
                        Fd_bits = (uint64_t)reinterpret_cast<uint32_t&>(fd);
                    }}, FloatMiscOp);
                    0x2: fsgnjx_s({{
                        uint32_t temp;
                        float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                        float fs2 = reinterpret_cast<float&>(temp = Fs2_bits);
                        float fd;

                        if (issignalingnan(fs1)) {
                            fd = numeric_limits<float>::signaling_NaN();
                            feclearexcept(FE_INVALID);
                        } else {
                            fd = fs1*(signbit(fs2) ? -1.0 : 1.0);
                        }
                        Fd_bits = (uint64_t)reinterpret_cast<uint32_t&>(fd);
                    }}, FloatMiscOp);
                }
                0x11: decode ROUND_MODE {
                    0x0: fsgnj_d({{
                        if (issignalingnan(Fs1)) {
                            Fd = numeric_limits<double>::signaling_NaN();
                            feclearexcept(FE_INVALID);
                        } else {
                            Fd = copysign(Fs1, Fs2);
                        }
                    }}, FloatMiscOp);
                    0x1: fsgnjn_d({{
                        if (issignalingnan(Fs1)) {
                            Fd = numeric_limits<double>::signaling_NaN();
                            feclearexcept(FE_INVALID);
                        } else {
                            Fd = copysign(Fs1, -Fs2);
                        }
                    }}, FloatMiscOp);
                    0x2: fsgnjx_d({{
                        if (issignalingnan(Fs1)) {
                            Fd = numeric_limits<double>::signaling_NaN();
                            feclearexcept(FE_INVALID);
                        } else {
                            Fd = Fs1*(signbit(Fs2) ? -1.0 : 1.0);
                        }
                    }}, FloatMiscOp);
                }
                0x14: decode ROUND_MODE {
                    0x0: fmin_s({{
                        uint32_t temp;
                        float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                        float fs2 = reinterpret_cast<float&>(temp = Fs2_bits);
                        float fd;

                        if (issignalingnan(fs2)) {
                            fd = fs1;
                            FFLAGS |= FloatInvalid;
                        } else if (issignalingnan(fs1)) {
                            fd = fs2;
                            FFLAGS |= FloatInvalid;
                        } else {
                            fd = fmin(fs1, fs2);
                        }
                        Fd_bits = (uint64_t)reinterpret_cast<uint32_t&>(fd);
                    }}, FloatCmpOp);
                    0x1: fmax_s({{
                        uint32_t temp;
                        float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                        float fs2 = reinterpret_cast<float&>(temp = Fs2_bits);
                        float fd;

                        if (issignalingnan(fs2)) {
                            fd = fs1;
                            FFLAGS |= FloatInvalid;
                        } else if (issignalingnan(fs1)) {
                            fd = fs2;
                            FFLAGS |= FloatInvalid;
                        } else {
                            fd = fmax(fs1, fs2);
                        }
                        Fd_bits = (uint64_t)reinterpret_cast<uint32_t&>(fd);
                    }}, FloatCmpOp);
                }
                0x15: decode ROUND_MODE {
                    0x0: fmin_d({{
                        if (issignalingnan(Fs2)) {
                            Fd = Fs1;
                            FFLAGS |= FloatInvalid;
                        } else if (issignalingnan(Fs1)) {
                            Fd = Fs2;
                            FFLAGS |= FloatInvalid;
                        } else {
                            Fd = fmin(Fs1, Fs2);
                        }
                    }}, FloatCmpOp);
                    0x1: fmax_d({{
                        if (issignalingnan(Fs2)) {
                            Fd = Fs1;
                            FFLAGS |= FloatInvalid;
                        } else if (issignalingnan(Fs1)) {
                            Fd = Fs2;
                            FFLAGS |= FloatInvalid;
                        } else {
                            Fd = fmax(Fs1, Fs2);
                        }
                    }}, FloatCmpOp);
                }
                0x20: fcvt_s_d({{
                    if (CONV_SGN != 1) {
                        fault = make_shared<IllegalInstFault>("CONV_SGN != 1",
                                                              machInst);
                    }
                    float fd;
                    if (issignalingnan(Fs1)) {
                        fd = numeric_limits<float>::quiet_NaN();
                        FFLAGS |= FloatInvalid;
                    } else {
                        fd = (float)Fs1;
                    }
                    Fd_bits = (uint64_t)reinterpret_cast<uint32_t&>(fd);
                }}, FloatCvtOp);
                0x21: fcvt_d_s({{
                    if (CONV_SGN != 0) {
                        fault = make_shared<IllegalInstFault>("CONV_SGN != 0",
                                                              machInst);
                    }
                    uint32_t temp;
                    float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);

                    if (issignalingnan(fs1)) {
                        Fd = numeric_limits<double>::quiet_NaN();
                        FFLAGS |= FloatInvalid;
                    } else {
                        Fd = (double)fs1;
                    }
                }}, FloatCvtOp);
                0x2c: fsqrt_s({{
                    if (RS2 != 0) {
                        fault = make_shared<IllegalInstFault>("source reg x1",
                                                              machInst);
                    }
                    uint32_t temp;
                    float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                    float fd;

                    if (issignalingnan(Fs1_sf)) {
                        FFLAGS |= FloatInvalid;
                    }
                    fd = sqrt(fs1);
                    Fd_bits = (uint64_t)reinterpret_cast<uint32_t&>(fd);
                }}, FloatSqrtOp);
                0x2d: fsqrt_d({{
                    if (RS2 != 0) {
                        fault = make_shared<IllegalInstFault>("source reg x1",
                                                              machInst);
                    }
                    Fd = sqrt(Fs1);
                }}, FloatSqrtOp);
                0x50: decode ROUND_MODE {
                    0x0: fle_s({{
                        uint32_t temp;
                        float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                        float fs2 = reinterpret_cast<float&>(temp = Fs2_bits);

                        if (std::isnan(fs1) || std::isnan(fs2)) {
                            FFLAGS |= FloatInvalid;
                            Rd = 0;
                        } else {
                            Rd = fs1 <= fs2 ? 1 : 0;
                        }
                    }}, FloatCmpOp);
                    0x1: flt_s({{
                        uint32_t temp;
                        float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                        float fs2 = reinterpret_cast<float&>(temp = Fs2_bits);

                        if (std::isnan(fs1) || std::isnan(fs2)) {
                            FFLAGS |= FloatInvalid;
                            Rd = 0;
                        } else {
                            Rd = fs1 < fs2 ? 1 : 0;
                        }
                    }}, FloatCmpOp);
                    0x2: feq_s({{
                        uint32_t temp;
                        float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                        float fs2 = reinterpret_cast<float&>(temp = Fs2_bits);

                        if (issignalingnan(fs1) || issignalingnan(fs2)) {
                            FFLAGS |= FloatInvalid;
                        }
                        Rd = fs1 == fs2 ? 1 : 0;
                    }}, FloatCmpOp);
                }
                0x51: decode ROUND_MODE {
                    0x0: fle_d({{
                        if (std::isnan(Fs1) || std::isnan(Fs2)) {
                            FFLAGS |= FloatInvalid;
                            Rd = 0;
                        } else {
                            Rd = Fs1 <= Fs2 ? 1 : 0;
                        }
                    }}, FloatCmpOp);
                    0x1: flt_d({{
                        if (std::isnan(Fs1) || std::isnan(Fs2)) {
                            FFLAGS |= FloatInvalid;
                            Rd = 0;
                        } else {
                            Rd = Fs1 < Fs2 ? 1 : 0;
                        }
                    }}, FloatCmpOp);
                    0x2: feq_d({{
                        if (issignalingnan(Fs1) || issignalingnan(Fs2)) {
                            FFLAGS |= FloatInvalid;
                        }
                        Rd = Fs1 == Fs2 ? 1 : 0;
                    }}, FloatCmpOp);
                }
                0x60: decode CONV_SGN {
                    0x0: fcvt_w_s({{
                        uint32_t temp;
                        float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);

                        if (std::isnan(fs1)) {
                            Rd_sd = numeric_limits<int32_t>::max();
                            FFLAGS |= FloatInvalid;
                        } else if (fs1 >= numeric_limits<int32_t>::max()) {
                            Rd_sd = numeric_limits<int32_t>::max();
                            FFLAGS |= FloatInvalid;
                        } else if (fs1 <= numeric_limits<int32_t>::min()) {
                            Rd_sd = numeric_limits<int32_t>::min();
                            FFLAGS |= FloatInvalid;
                        } else {
                            Rd_sd = (int32_t)fs1;
                        }
                    }}, FloatCvtOp);
                    0x1: fcvt_wu_s({{
                        uint32_t temp;
                        float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);

                        if (std::isnan(fs1)) {
                            Rd = numeric_limits<uint64_t>::max();
                            FFLAGS |= FloatInvalid;
                        } else if (fs1 < 0.0) {
                            Rd = 0;
                            FFLAGS |= FloatInvalid;
                        } else if (fs1 > numeric_limits<uint32_t>::max()) {
                            Rd = numeric_limits<uint64_t>::max();
                            FFLAGS |= FloatInvalid;
                        } else {
                            Rd = (uint32_t)fs1;
                        }
                    }}, FloatCvtOp);
                    0x2: fcvt_l_s({{
                        uint32_t temp;
                        float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);

                        if (std::isnan(fs1)) {
                            Rd_sd = numeric_limits<int64_t>::max();
                            FFLAGS |= FloatInvalid;
                        } else if (fs1 > numeric_limits<int64_t>::max()) {
                            Rd_sd = numeric_limits<int64_t>::max();
                            FFLAGS |= FloatInvalid;
                        } else if (fs1 < numeric_limits<int64_t>::min()) {
                            Rd_sd = numeric_limits<int64_t>::min();
                            FFLAGS |= FloatInvalid;
                        } else {
                            Rd_sd = (int64_t)fs1;
                        }
                    }}, FloatCvtOp);
                    0x3: fcvt_lu_s({{
                        uint32_t temp;
                        float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);

                        if (std::isnan(fs1)) {
                            Rd = numeric_limits<uint64_t>::max();
                            FFLAGS |= FloatInvalid;
                        } else if (fs1 < 0.0) {
                            Rd = 0;
                            FFLAGS |= FloatInvalid;
                        } else if (fs1 > numeric_limits<uint64_t>::max()) {
                            Rd = numeric_limits<uint64_t>::max();
                            FFLAGS |= FloatInvalid;
                        } else {
                            Rd = (uint64_t)fs1;
                        }
                    }}, FloatCvtOp);
                }
                0x61: decode CONV_SGN {
                    0x0: fcvt_w_d({{
                        if (std::isnan(Fs1)) {
                            Rd_sd = numeric_limits<int32_t>::max();
                            FFLAGS |= FloatInvalid;
                        } else if (Fs1 > numeric_limits<int32_t>::max()) {
                            Rd_sd = numeric_limits<int32_t>::max();
                            FFLAGS |= FloatInvalid;
                        } else if (Fs1 < numeric_limits<int32_t>::min()) {
                            Rd_sd = numeric_limits<int32_t>::min();
                            FFLAGS |= FloatInvalid;
                        } else {
                            Rd_sd = (int32_t)Fs1;
                        }
                    }}, FloatCvtOp);
                    0x1: fcvt_wu_d({{
                        if (std::isnan(Fs1)) {
                            Rd = numeric_limits<uint64_t>::max();
                            FFLAGS |= FloatInvalid;
                        } else if (Fs1 < 0) {
                            Rd = 0;
                            FFLAGS |= FloatInvalid;
                        } else if (Fs1 > numeric_limits<uint32_t>::max()) {
                            Rd = numeric_limits<uint64_t>::max();
                            FFLAGS |= FloatInvalid;
                        } else {
                            Rd = (uint32_t)Fs1;
                        }
                    }}, FloatCvtOp);
                    0x2: fcvt_l_d({{
                        if (std::isnan(Fs1)) {
                            Rd_sd = numeric_limits<int64_t>::max();
                            FFLAGS |= FloatInvalid;
                        } else if (Fs1 > numeric_limits<int64_t>::max()) {
                            Rd_sd = numeric_limits<int64_t>::max();
                            FFLAGS |= FloatInvalid;
                        } else if (Fs1 < numeric_limits<int64_t>::min()) {
                            Rd_sd = numeric_limits<int64_t>::min();
                            FFLAGS |= FloatInvalid;
                        } else {
                            Rd_sd = Fs1;
                        }
                    }}, FloatCvtOp);
                    0x3: fcvt_lu_d({{
                        if (std::isnan(Fs1)) {
                            Rd = numeric_limits<uint64_t>::max();
                            FFLAGS |= FloatInvalid;
                        } else if (Fs1 < 0) {
                            Rd = 0;
                            FFLAGS |= FloatInvalid;
                        } else if (Fs1 > numeric_limits<uint64_t>::max()) {
                            Rd = numeric_limits<uint64_t>::max();
                            FFLAGS |= FloatInvalid;
                        } else {
                            Rd = Fs1;
                        }
                    }}, FloatCvtOp);
                }
                0x68: decode CONV_SGN {
                    0x0: fcvt_s_w({{
                        float temp = (float)Rs1_sw;
                        Fd_bits = (uint64_t)reinterpret_cast<uint32_t&>(temp);
                    }}, FloatCvtOp);
                    0x1: fcvt_s_wu({{
                        float temp = (float)Rs1_uw;
                        Fd_bits = (uint64_t)reinterpret_cast<uint32_t&>(temp);
                    }}, FloatCvtOp);
                    0x2: fcvt_s_l({{
                        float temp = (float)Rs1_sd;
                        Fd_bits = (uint64_t)reinterpret_cast<uint32_t&>(temp);
                    }}, FloatCvtOp);
                    0x3: fcvt_s_lu({{
                        float temp = (float)Rs1;
                        Fd_bits = (uint64_t)reinterpret_cast<uint32_t&>(temp);
                    }}, FloatCvtOp);
                }
                0x69: decode CONV_SGN {
                    0x0: fcvt_d_w({{
                        Fd = (double)Rs1_sw;
                    }}, FloatCvtOp);
                    0x1: fcvt_d_wu({{
                        Fd = (double)Rs1_uw;
                    }}, FloatCvtOp);
                    0x2: fcvt_d_l({{
                        Fd = (double)Rs1_sd;
                    }}, FloatCvtOp);
                    0x3: fcvt_d_lu({{
                        Fd = (double)Rs1;
                    }}, FloatCvtOp);
                }
                0x70: decode ROUND_MODE {
                    0x0: fmv_x_s({{
                        Rd = (uint32_t)Fs1_bits;
                        if ((Rd&0x80000000) != 0) {
                            Rd |= (0xFFFFFFFFULL << 32);
                        }
                    }}, FloatCvtOp);
                    0x1: fclass_s({{
                        uint32_t temp;
                        float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                        switch (fpclassify(fs1)) {
                          case FP_INFINITE:
                            if (signbit(fs1)) {
                                Rd = 1 << 0;
                            } else {
                                Rd = 1 << 7;
                            }
                            break;
                          case FP_NAN:
                            if (issignalingnan(fs1)) {
                                Rd = 1 << 8;
                            } else {
                                Rd = 1 << 9;
                            }
                            break;
                          case FP_ZERO:
                            if (signbit(fs1)) {
                                Rd = 1 << 3;
                            } else {
                                Rd = 1 << 4;
                            }
                            break;
                          case FP_SUBNORMAL:
                            if (signbit(fs1)) {
                                Rd = 1 << 2;
                            } else {
                                Rd = 1 << 5;
                            }
                            break;
                          case FP_NORMAL:
                            if (signbit(fs1)) {
                                Rd = 1 << 1;
                            } else {
                                Rd = 1 << 6;
                            }
                            break;
                          default:
                            panic("Unknown classification for operand.");
                            break;
                        }
                    }}, FloatMiscOp);
                }
                0x71: decode ROUND_MODE {
                    0x0: fmv_x_d({{
                        Rd = Fs1_bits;
                    }}, FloatCvtOp);
                    0x1: fclass_d({{
                        switch (fpclassify(Fs1)) {
                          case FP_INFINITE:
                            if (signbit(Fs1)) {
                                Rd = 1 << 0;
                            } else {
                                Rd = 1 << 7;
                            }
                            break;
                          case FP_NAN:
                            if (issignalingnan(Fs1)) {
                                Rd = 1 << 8;
                            } else {
                                Rd = 1 << 9;
                            }
                            break;
                          case FP_ZERO:
                            if (signbit(Fs1)) {
                                Rd = 1 << 3;
                            } else {
                                Rd = 1 << 4;
                            }
                            break;
                          case FP_SUBNORMAL:
                            if (signbit(Fs1)) {
                                Rd = 1 << 2;
                            } else {
                                Rd = 1 << 5;
                            }
                            break;
                          case FP_NORMAL:
                            if (signbit(Fs1)) {
                                Rd = 1 << 1;
                            } else {
                                Rd = 1 << 6;
                            }
                            break;
                          default:
                            panic("Unknown classification for operand.");
                            break;
                        }
                    }}, FloatMiscOp);
                }
                0x78: fmv_s_x({{
                    Fd_bits = (uint64_t)Rs1_uw;
                }}, FloatCvtOp);
                0x79: fmv_d_x({{
                    Fd_bits = Rs1;
                }}, FloatCvtOp);
            }
        }

        // +--> VINT
        0x15: decode FUNCT3 {
            // OPIVV
            0x0: decode FUNCT6 {
                // vadd
                0x0: decode VM {
                    // vadd_vv
                    0x1: VOPIVV::vadd_vv({{
                        DPRINTF(RiscvVector, "vadd_vv len %d\n", VLEN);
                        for (auto i = 0; i < VLEN; i++) {
                            DPRINTF(RiscvVector, "vecops %llu + %llu\n", Vs2_ud[i], Vs1_ud[i]);
                            Vd_ud[i] = Vs2_ud[i] + Vs1_ud[i];
                            
                        }
                    }}, flags=['SimdAddOp']);

                    // vadd_vvm
                    0x0: VOPIVV::vadd_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] + Vs1_ud[i];
                            }
                        }
                    }}, flags=['SimdAddOp']);
                }

                // vsub
                0x2: decode VM {
                    // vsub_vv
                    0x1: VOPIVV::vsub_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] - Vs1_ud[i];
                        }
                    }}, flags=['SimdAddOp']);

                    // vsub_vvm
                    0x0: VOPIVV::vsub_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] - Vs1_ud[i];
                            }
                        }
                    }}, flags=['SimdAddOp']);
                }

                // vminu
                0x4: decode VM {
                    // vminu_vv
                    0x1: VOPIVV::vminu_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = std::min(Vs2_ud[i] , Vs1_ud[i]);
                        }
                    }}, flags=['SimdCmpOp']);

                    // vminu_vvm
                    0x0: VOPIVV::vminu_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = std::min(Vs2_ud[i] , Vs1_ud[i]);
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmin
                0x5: decode VM {
                    // vmin_vv
                    0x1: VOPIVV::vmin_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = std::min(Vs2_sd[i] , Vs1_sd[i]);
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmin_vvm
                    0x0: VOPIVV::vmin_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_sd[i] = std::min(Vs2_sd[i] , Vs1_sd[i]);
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmaxu
                0x6: decode VM {
                    // vmaxu_vv
                    0x1: VOPIVV::vmaxu_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = std::max(Vs2_ud[i] , Vs1_ud[i]);
                        }
                     }}, flags=['SimdCmpOp']);

                    // vmaxu_vvm
                    0x0: VOPIVV::vmaxu_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = std::max(Vs2_ud[i] , Vs1_ud[i]);
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmax
                0x7: decode VM {
                    // vmax_vv
                    0x1: VOPIVV::vmax_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = std::max(Vs2_sd[i] , Vs1_sd[i]);
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmax_vvm
                    0x0: VOPIVV::vmax_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_sd[i] = std::max(Vs2_sd[i] , Vs1_sd[i]);
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vand
                0x9: decode VM {
                    // vand_vv
                    0x1: VOPIVV::vand_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] & Vs1_ud[i];
                        }
                    }}, flags=['SimdAluOp']);

                    // vand_vvm
                    0x0: VOPIVV::vand_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] & Vs1_ud[i];
                            }
                        }
                    }}, flags=['SimdAluOp']);
                }

                // vor
                0xa: decode VM {
                    // vor_vv
                    0x1: VOPIVV::vor_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] | Vs1_ud[i];
                        }
                    }}, flags=['SimdAluOp']);

                    // vor_vvm
                    0x0: VOPIVV::vor_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] | Vs1_ud[i];
                            }
                        }
                    }}, flags=['SimdAluOp']);
                }

                // vxor
                0x0b: decode VM {
                    // vxor_vv
                    0x1: VOPIVV::vxor_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] ^ Vs1_ud[i];
                        }
                    }}, flags=['SimdAluOp']);

                    // vxor_vvm
                    0x0: VOPIVV::vxor_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] ^ Vs1_ud[i];
                            }
                        }
                    }}, flags=['SimdAluOp']);
                }

                // vmerge/vmv
                0x17: decode VM {
                    // vmv
                    0x1: VOPIVV::vmv_v_v({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs1_ud[i];
                        }
                    }}, flags=['SimdAluOp']);

                    // vmerge_vv
                    0x0: VOPIVV::vmerge_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs1_ud[i];
                            } else {
                                Vd_ud[i] = Vs2_ud[i];
                            }
                        }
                    }}, flags=['SimdAluOp']);
                }

                // vmseq
                0x18: decode VM {
                    // vmseq_vv
                    0x1: VOPIVV::vmseq_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] == Vs1_ud[i];
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmseq_vvm
                    0x0: VOPIVV::vmseq_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] == Vs1_ud[i];
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmsne
                0x19: decode VM {
                    // vmsne_vv
                    0x1: VOPIVV::vmsne_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] != Vs1_ud[i];
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmsne_vvm
                    0x0: VOPIVV::vmsne_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] != Vs1_ud[i];
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmsltu
                0x1a: decode VM {
                    // vmsltu_vv
                    0x1: VOPIVV::vmsltu_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] < Vs1_ud[i];
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmsltu_vvm
                    0x0: VOPIVV::vmsltu_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] < Vs1_ud[i];
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmslt
                0x1b: decode VM {
                    // vmslt_vv
                    0x1: VOPIVV::vmslt_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = Vs2_sd[i] < Vs1_sd[i];
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmslt_vvm
                    0x0: VOPIVV::vmslt_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_sd[i] = Vs2_sd[i] < Vs1_sd[i];
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmsleu
                0x1c: decode VM {
                    // vmsleu_vv
                    0x1: VOPIVV::vmsleu_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] <= Vs1_ud[i];
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmsleu_vvm
                    0x0: VOPIVV::vmsleu_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] <= Vs1_ud[i];
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmsle
                0x1d: decode VM {
                    // vmsle_vv
                    0x1: VOPIVV::vmsle_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = Vs2_sd[i] <= Vs1_sd[i];
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmsle_vvm
                    0x0: VOPIVV::vmsle_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_sd[i] = Vs2_sd[i] <= Vs1_sd[i];
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmsgtu
                0x1e: decode VM {
                    // vmsgtu_vv
                    0x1: VOPIVV::vmsgtu_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] > Vs1_ud[i];
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmsgtu_vvm
                    0x0: VOPIVV::vmsgtu_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] > Vs1_ud[i];
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmsgt
                0x1f: decode VM {
                    // vmsgt_vv
                    0x1: VOPIVV::vmsgt_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = Vs2_sd[i] > Vs1_sd[i];
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmsgt_vvm
                    0x0: VOPIVV::vmsgt_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_sd[i] = Vs2_sd[i] > Vs1_sd[i];
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vsll
                0x25: decode VM {
                    // vsll_vv
                    0x1: VOPIVV::vsll_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] << (Vs1_ud[i] & (64 - 1));
                        }
                    }}, flags=['SimdShiftOp']);

                    // vsll_vvm
                    0x0: VOPIVV::vsll_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] << (Vs1_ud[i] & (64 - 1));
                            }
                        }
                    }}, flags=['SimdShiftOp']);
                }

                // vsrl
                0x28: decode VM {
                    // vsrl_vv
                    0x1: VOPIVV::vsrl_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] >> (Vs1_ud[i] & (64 - 1));
                        }
                    }}, flags=['SimdShiftOp']);

                    // vsrl_vvm
                    0x0: VOPIVV::vsrl_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] >> (Vs1_ud[i] & (64 - 1));
                            }
                        }
                    }}, flags=['SimdShiftOp']);
                }

                // vsra
                0x29: decode VM {
                    // vsra_vv
                    0x1: VOPIVV::vsra_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = Vs2_sd[i] >> (Vs1_sd[i] & (64 - 1));
                        }
                    }}, flags=['SimdShiftOp']);

                    // vsra_vvm
                    0x0: VOPIVV::vsra_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_sd[i] & 0x1llu) {
                                Vd_sd[i] = Vs2_sd[i] >> (Vs1_sd[i] & (64 - 1));
                            }
                        }
                    }}, flags=['SimdShiftOp']);
                }
            }

            // OPFVV
            0x1: decode FUNCT6 {
                // vfadd
                0x0: decode VM {
                    // vfadd_vv
                    0x1: VOPFVV::vfadd_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            uint32_t temp;
                            float fs1 = reinterpret_cast<float&>(temp = Vs1_ud[i]);
                            float fs2 = reinterpret_cast<float&>(temp = Vs2_ud[i]);
                            float res = fs2 + fs1;
                            DPRINTF(RiscvVector, "%f = %f + %f\n", res, fs2, fs1);
                            Vd_ud[i] = reinterpret_cast<uint32_t&>(res);
                        }
                    }}, flags=['SimdFloatAddOp']);

                    // vfadd_vvm
                    0x0: VOPFVV::vfadd_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                uint32_t temp;
                                float fs1 = reinterpret_cast<float&>(temp = Vs1_ud[i]);
                                float fs2 = reinterpret_cast<float&>(temp = Vs2_ud[i]);
                                float res = fs2 + fs1;
                                Vd_ud[i] = reinterpret_cast<uint32_t&>(res);
                            }
                        }
                    }}, flags=['SimdFloatAddOp']);
                }

                // vfredsum
                0x01: decode VM {
                    // vfredsum_vs
                    0x1: VOPFVV::vfredsum_vs({{
                        uint32_t temp;
                        float fs1 = reinterpret_cast<float&>(temp = Vs1_ud[0]);
                        float res = fs1;
                        for (auto i = 0; i < VLEN; i++) {
                            float fs2 = reinterpret_cast<float&>(temp = Vs2_ud[i]);
                            res += fs2;
                            // DPRINTF(RiscvVector, "sum now %f\n", res);
                        }
                        Vd_ud[0] = reinterpret_cast<uint32_t&>(res);
                    }}, flags=['SimdFloatReduceAddOp']);

                    // vfredsum_vsm
                    0x0: VOPFVV::vfredsum_vsm({{
                        uint32_t temp;
                        float fs1 = reinterpret_cast<float&>(temp = Vs1_ud[0]);
                        float res = fs1;
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                float fs2 = reinterpret_cast<float&>(temp = Vs2_ud[i]);
                                res += fs2;
                            }
                        }
                        Vd_ud[0] = reinterpret_cast<uint32_t&>(res);
                    }}, flags=['SimdFloatReduceAddOp']);
                }

                // vfsub
                0x2: decode VM {
                    // vfsub_vv
                    0x1: VOPFVV::vfsub_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            uint32_t temp;
                            float fs1 = reinterpret_cast<float&>(temp = Vs1_ud[i]);
                            float fs2 = reinterpret_cast<float&>(temp = Vs2_ud[i]);
                            float res = fs2 - fs1;
                            DPRINTF(RiscvVector, "%f = %f - %f\n", res, fs2, fs1);
                            Vd_ud[i] = reinterpret_cast<uint32_t&>(res);
                        }
                    }}, flags=['SimdFloatAddOp']);

                    // vfsub_vvm
                    0x0: VOPFVV::vfsub_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                uint32_t temp;
                                float fs1 = reinterpret_cast<float&>(temp = Vs1_ud[i]);
                                float fs2 = reinterpret_cast<float&>(temp = Vs2_ud[i]);
                                float res = fs2 - fs1;
                                Vd_ud[i] = reinterpret_cast<uint32_t&>(res);
                            }
                        }
                    }}, flags=['SimdFloatAddOp']);
                }

                // // vminu
                // 0x4: decode VM {
                //     // vminu_vv
                //     0x1: VOPIVV::vminu_vv({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_ud[i] = std::min(Vs2_ud[i] , Vs1_ud[i]);
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vminu_vvm
                //     0x0: VOPIVV::vminu_vvm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_ud[i] = std::min(Vs2_ud[i] , Vs1_ud[i]);
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // // vmin
                // 0x5: decode VM {
                //     // vmin_vv
                //     0x1: VOPIVV::vmin_vv({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_sd[i] = std::min(Vs2_sd[i] , Vs1_sd[i]);
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vmin_vvm
                //     0x0: VOPIVV::vmin_vvm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_sd[i] = std::min(Vs2_sd[i] , Vs1_sd[i]);
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // // vmaxu
                // 0x6: decode VM {
                //     // vmaxu_vv
                //     0x1: VOPIVV::vmaxu_vv({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_ud[i] = std::max(Vs2_ud[i] , Vs1_ud[i]);
                //         }
                //      }}, flags=['SimdCmpOp']);

                //     // vmaxu_vvm
                //     0x0: VOPIVV::vmaxu_vvm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_ud[i] = std::max(Vs2_ud[i] , Vs1_ud[i]);
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // // vmax
                // 0x7: decode VM {
                //     // vmax_vv
                //     0x1: VOPIVV::vmax_vv({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_sd[i] = std::max(Vs2_sd[i] , Vs1_sd[i]);
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vmax_vvm
                //     0x0: VOPIVV::vmax_vvm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_sd[i] = std::max(Vs2_sd[i] , Vs1_sd[i]);
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // vmv
                0x10: decode VM {
                    // vmv
                    0x1: VOPFVV::vfmv_f_s({{
                        uint32_t temp;
                        float fd = reinterpret_cast<float&>(temp = Vs2_ud[0]);
                        Fd_bits = (uint64_t)reinterpret_cast<uint32_t&>(fd);
                        // DPRINTF(RiscvVector, "mv %f\n", reinterpret_cast<float&>(temp = Vs2_ud[0]));
                    }}, flags=['SimdFloatAluOp']);
                }

                // // vmerge/vmv
                // 0x17: decode VM {
                //     // vmv
                //     0x1: VOPIVV::vmv_v_v({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_ud[i] = Vs1_ud[i];
                //         }
                //     }}, flags=['SimdAluOp']);

                //     // vmerge_vv
                //     0x0: VOPIVV::vmerge_vvm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_ud[i] = Vs1_ud[i];
                //             } else {
                //                 Vd_ud[i] = Vs2_ud[i];
                //             }
                //         }
                //     }}, flags=['SimdAluOp']);
                // }

                // // vmseq
                // 0x18: decode VM {
                //     // vmseq_vv
                //     0x1: VOPIVV::vmseq_vv({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_ud[i] = Vs2_ud[i] == Vs1_ud[i];
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vmseq_vvm
                //     0x0: VOPIVV::vmseq_vvm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_ud[i] = Vs2_ud[i] == Vs1_ud[i];
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // // vmsne
                // 0x19: decode VM {
                //     // vmsne_vv
                //     0x1: VOPIVV::vmsne_vv({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_ud[i] = Vs2_ud[i] != Vs1_ud[i];
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vmsne_vvm
                //     0x0: VOPIVV::vmsne_vvm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_ud[i] = Vs2_ud[i] != Vs1_ud[i];
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // // vmsltu
                // 0x1a: decode VM {
                //     // vmsltu_vv
                //     0x1: VOPIVV::vmsltu_vv({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_ud[i] = Vs2_ud[i] < Vs1_ud[i];
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vmsltu_vvm
                //     0x0: VOPIVV::vmsltu_vvm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_ud[i] = Vs2_ud[i] < Vs1_ud[i];
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // // vmslt
                // 0x1b: decode VM {
                //     // vmslt_vv
                //     0x1: VOPIVV::vmslt_vv({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_sd[i] = Vs2_sd[i] < Vs1_sd[i];
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vmslt_vvm
                //     0x0: VOPIVV::vmslt_vvm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_sd[i] = Vs2_sd[i] < Vs1_sd[i];
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // // vmsleu
                // 0x1c: decode VM {
                //     // vmsleu_vv
                //     0x1: VOPIVV::vmsleu_vv({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_ud[i] = Vs2_ud[i] <= Vs1_ud[i];
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vmsleu_vvm
                //     0x0: VOPIVV::vmsleu_vvm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_ud[i] = Vs2_ud[i] <= Vs1_ud[i];
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // // vmsle
                // 0x1d: decode VM {
                //     // vmsle_vv
                //     0x1: VOPIVV::vmsle_vv({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_sd[i] = Vs2_sd[i] <= Vs1_sd[i];
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vmsle_vvm
                //     0x0: VOPIVV::vmsle_vvm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_sd[i] = Vs2_sd[i] <= Vs1_sd[i];
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // // vmsgtu
                // 0x1e: decode VM {
                //     // vmsgtu_vv
                //     0x1: VOPIVV::vmsgtu_vv({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_ud[i] = Vs2_ud[i] > Vs1_ud[i];
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vmsgtu_vvm
                //     0x0: VOPIVV::vmsgtu_vvm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_ud[i] = Vs2_ud[i] > Vs1_ud[i];
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // // vmsgt
                // 0x1f: decode VM {
                //     // vmsgt_vv
                //     0x1: VOPIVV::vmsgt_vv({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_sd[i] = Vs2_sd[i] > Vs1_sd[i];
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vmsgt_vvm
                //     0x0: VOPIVV::vmsgt_vvm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_sd[i] = Vs2_sd[i] > Vs1_sd[i];
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // vfdiv
                0x20: decode VM {
                    // vfdiv_vv
                    0x1: VOPFVV::vfdiv_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            uint32_t temp;
                            float fs1 = reinterpret_cast<float&>(temp = Vs1_sd[i]);
                            float fs2 = reinterpret_cast<float&>(temp = Vs2_sd[i]);
                            float res = fs2 / fs1;
                            Vd_sd[i] = reinterpret_cast<uint32_t&>(res);
                        }
                    }}, flags=['SimdFloatDivOp']);

                    // vfdiv_vvm
                    0x0: VOPFVV::vfdiv_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                uint32_t temp;
                                float fs1 = reinterpret_cast<float&>(temp = Vs1_sd[i]);
                                float fs2 = reinterpret_cast<float&>(temp = Vs2_sd[i]);
                                float res = fs2 / fs1;
                                Vd_sd[i] = reinterpret_cast<uint32_t&>(res);
                            }
                        }
                    }}, flags=['SimdFloatDivOp']);
                }

                // vfmul
                0x24: decode VM {
                    // vfmul_vv
                    0x1: VOPFVV::vfmul_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            uint32_t temp;
                            float fs1 = reinterpret_cast<float&>(temp = Vs1_sd[i]);
                            float fs2 = reinterpret_cast<float&>(temp = Vs2_sd[i]);
                            float res = fs2 * fs1;
                            // DPRINTF(RiscvVector, "%f = %f * %f\n", res, fs1, fs2);
                            Vd_sd[i] = reinterpret_cast<uint32_t&>(res);
                        }
                    }}, flags=['SimdFloatMultOp']);

                    // vfmul_vvm
                    0x0: VOPFVV::vfmul_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                uint32_t temp;
                                float fs1 = reinterpret_cast<float&>(temp = Vs1_sd[i]);
                                float fs2 = reinterpret_cast<float&>(temp = Vs2_sd[i]);
                                float res = fs2 * fs1;
                                Vd_sd[i] = reinterpret_cast<uint32_t&>(res);
                            }
                        }
                    }}, flags=['SimdFloatMultOp']);
                }
            }

            // OPMVV
            0x2: decode FUNCT6 {
                // vredsum
                0x00: decode VM {
                    // vredsum
                    0x1: VOPIVV::vredsum({{
                        auto res = Vs1_ud[0];
                        for (auto i = 0; i < VLEN; i++) {
                            res += Vs2_ud[i];
                        }
                        Vd_ud[0] = res;
                    }}, flags=['SimdReduceAddOp']);

                    // vredsum_m
                    0x0: VOPIVV::vredsum_m({{
                        auto res = Vs1_ud[0];
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                res += Vs2_ud[i];
                            }
                        }
                        Vd_ud[0] = res;
                    }}, flags=['SimdReduceAddOp']);
                }

                // VWXUNARY0
                0x10: decode VS1 {
                    0x00: VOPMVV::vmv_x_s({{
                        Rd_ud = Vs2_ud[0];
                    }}, flags=['SimdAluOp']);

                    0x10: decode VM {
                        0x1: VOPMVV::vpopc({{
                            auto res = 0;
                            for (auto i = 0; i < VLEN; i++) {
                                res += (Vs2_ud[i] & 0x1);
                            }
                            Rd_ud = res;
                        }}, flags=['SimdReduceAddOp']);

                        0x0: VOPMVV::vpopc_m({{
                            auto res = 0;
                            for (auto i = 0; i < VLEN; i++) {
                                res += (Vs2_ud[i] & Vm_ud[i] & 0x1);
                            }
                            Rd_ud = res;
                        }}, flags=['SimdReduceAddOp']);
                    }
                }

                // vmul
                0x25: decode VM {
                    // vmul_vv
                    0x1: VOPIVV::vmul_vv({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = Vs2_sd[i] * Vs1_sd[i];
                        }
                    }}, flags=['SimdMultOp']);

                    // vmul_vvm
                    0x0: VOPIVV::vmul_vvm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_sd[i] = Vs2_sd[i] * Vs1_sd[i];
                            }
                        }
                    }}, flags=['SimdMultOp']);
                }
            }

            // OPIVI
            0x3: decode FUNCT6 {
                // vadd
                0x0: decode VM {
                    // vadd_vi
                    0x1: VOPIVI::vadd_vi({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = Vs2_sd[i] + imm;
                        }
                    }}, flags=['SimdAddOp']);

                    // vadd_vim
                    0x0: VOPIVI::vadd_vim({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_sd[i] = Vs2_sd[i] + imm;
                            }
                        }
                    }}, flags=['SimdAddOp']);
                }

                // vrsub
                0x3: decode VM {
                    // vrsub_vi
                    0x1: VOPIVI::vrsub_vi({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = imm - Vs2_sd[i];
                        }
                    }}, flags=['SimdAddOp']);

                    // vrsub_vim
                    0x0: VOPIVI::vrsub_vim({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_sd[i] = imm - Vs2_sd[i];
                            }
                        }
                    }}, flags=['SimdAddOp']);
                }

                // vand
                0x9: decode VM {
                    // vand_vi
                    0x1: VOPIVI::vand_vi({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] & imm;
                        }
                    }}, flags=['SimdAluOp']);

                    // vand_vim
                    0x0: VOPIVI::vand_vim({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] & imm;
                            }
                        }
                    }}, flags=['SimdAluOp']);
                }

                // vor
                0xa: decode VM {
                    // vor_vi
                    0x1: VOPIVI::vor_vi({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] | imm;
                        }
                    }}, flags=['SimdAluOp']);

                    // vor_vim
                    0x0: VOPIVI::vor_vim({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] | imm;
                            }
                        }
                    }}, flags=['SimdAluOp']);
                }

                // vxor
                0x0b: decode VM {
                    // vxor_vi
                    0x1: VOPIVI::vxor_vi({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] ^ imm;
                        }
                    }}, flags=['SimdAluOp']);

                    // vxor_vim
                    0x0: VOPIVI::vxor_vim({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] ^ imm;
                            }
                        }
                    }}, flags=['SimdAluOp']);
                }

                // vslideup
                0x0e: decode VM {
                    // vslideup_vi
                    0x1: VOPIVI::vslideup_vi({{
                        for (auto i = imm; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i - imm];
                        }
                    }}, flags=['SimdAluOp']);

                    // vslideup_vim
                    0x0: VOPIVI::vslideup_vim({{
                        for (auto i = imm; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i - imm];
                            }
                        }
                    }}, flags=['SimdAluOp']);
                }

                // vslidedown
                0x0f: decode VM {
                    // vslidedown_vi
                    0x1: VOPIVI::vslidedown_vi({{
                        auto max = xc->maxVectorLength();
                        for (auto i = 0; i < VLEN; i++) {
                            if ((i + imm) < max) {
                                Vd_ud[i] = Vs2_ud[i + imm];
                            } else {
                                Vd_ud[i] = 0;
                            }
                        }
                    }}, flags=['SimdAluOp']);

                    // vslidedown_vim
                    0x0: VOPIVI::vslidedown_vim({{
                        auto max = xc->maxVectorLength();
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                if ((i + imm) < max) {
                                    Vd_ud[i] = Vs2_ud[i + imm];
                                } else {
                                    Vd_ud[i] = 0;
                                }
                            }
                        }
                    }}, flags=['SimdAluOp']);
                }

                // vmerge/vmv
                0x17: decode VM {
                    // vmv
                    0x1: VOPIVI::vmv_v_i({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = imm;
                        }
                    }}, flags=['SimdAluOp']);

                    // vmerge_vi
                    0x0: VOPIVI::vmerge_vim({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = imm;
                            } else {
                                Vd_ud[i] = Vs2_ud[i];
                            }
                        }
                    }}, flags=['SimdAluOp']);
                }

                // vmseq
                0x18: decode VM {
                    // vmseq_vi
                    0x1: VOPIVI::vmseq_vi({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] == imm;
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmseq_vim
                    0x0: VOPIVI::vmseq_vim({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_sd[i] == imm;
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmsne
                0x19: decode VM {
                    // vmsne_vi
                    0x1: VOPIVI::vmsne_vi({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] != imm;
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmsne_vim
                    0x0: VOPIVI::vmsne_vim({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] != imm;
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmsltu
                0x1a: decode VM {
                    // vmsltu_vi
                    0x1: VOPIVI::vmsltu_vi({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] < imm;
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmsltu_vim
                    0x0: VOPIVI::vmsltu_vim({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] < imm;
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmslt
                0x1b: decode VM {
                    // vmslt_vi
                    0x1: VOPIVI::vmslt_vi({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = Vs2_sd[i] < imm;
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmslt_vim
                    0x0: VOPIVI::vmslt_vim({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_sd[i] = Vs2_sd[i] < imm;
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmsleu
                0x1c: decode VM {
                    // vmsleu_vi
                    0x1: VOPIVI::vmsleu_vi({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] <= imm;
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmsleu_vim
                    0x0: VOPIVI::vmsleu_vim({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] <= imm;
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmsle
                0x1d: decode VM {
                    // vmsle_vi
                    0x1: VOPIVI::vmsle_vi({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = Vs2_sd[i] <= imm;
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmsle_vim
                    0x0: VOPIVI::vmsle_vim({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_sd[i] = Vs2_sd[i] <= imm;
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmsgtu
                0x1e: decode VM {
                    // vmsgtu_vi
                    0x1: VOPIVI::vmsgtu_vi({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] >  imm;
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmsgtu_vim
                    0x0: VOPIVI::vmsgtu_vim({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] >  imm;
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmsgt
                0x1f: decode VM {
                    // vmsgt_vi
                    0x1: VOPIVI::vmsgt_vi({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = Vs2_sd[i] >  imm;
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmsgt_vim
                    0x0: VOPIVI::vmsgt_vim({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_sd[i] = Vs2_sd[i] >  imm;
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vsll
                0x25: decode VM {
                    // vsll_vi
                    0x1: VOPIVI::vsll_vi({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] << (imm & (64 - 1));
                        }
                    }}, flags=['SimdShiftOp']);

                    // vsll_vim
                    0x0: VOPIVI::vsll_vim({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] << (imm & (64 - 1));
                            }
                        }
                    }}, flags=['SimdShiftOp']);
                }

                // vmv<1,2,4,8>r.v
                0x27: decode VM {
                    0x1: decode SIMM5 {
                        0x0: VOPIVI::vmv1r_v({{
                            // DPRINTF(RiscvVector, "start vmv1r len %d\n", VLEN);
                            //assert(false); // LMUL=1 supported
                            for (auto i = 0; i < VLEN; i++) {
                                Vd_ud[i] = Vs2_ud[i];
                                // uint32_t temp;
                                // float vs2f = reinterpret_cast<float&>(temp = Vs2_ud[i]);
                                // DPRINTF(RiscvVector, "mv %f\n", vs2f);
                            }
                        }}, flags=['SimdAluOp']);

                        0x1: VOPIVI::vmv2r_v({{
                            assert(false); // TODO LMUL not support
                            for (auto i = 0; i < VLEN; i++) {
                                Vd_ud[i] = Vs2_ud[i];
                            }
                        }}, flags=['SimdAluOp']);

                        0x3: VOPIVI::vmv4r_v({{
                            assert(false); // TODO LMUL not support
                            for (auto i = 0; i < VLEN; i++) {
                                Vd_ud[i] = Vs2_ud[i];
                            }
                        }}, flags=['SimdAluOp']);


                        0x7: VOPIVI::vmv8r_v({{
                            assert(false); // TODO LMUL not support
                            for (auto i = 0; i < VLEN; i++) {
                                Vd_ud[i] = Vs2_ud[i];
                            }
                        }}, flags=['SimdAluOp']);
                    }
                }

                // vsrl
                0x28: decode VM {
                    // vsrl_vi
                    0x1: VOPIVI::vsrl_vi({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] >> (imm & (64 - 1));
                        }
                    }}, flags=['SimdShiftOp']);

                    // vsrl_vim
                    0x0: VOPIVI::vsrl_vim({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] >> (imm & (64 - 1));
                            }
                        }
                    }}, flags=['SimdShiftOp']);
                }

                // vsra
                0x29: decode VM {
                    // vsra_vi
                    0x1: VOPIVI::vsra_vi({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = Vs2_sd[i] >> (imm & (64 - 1));
                        }
                    }}, flags=['SimdShiftOp']);

                    // vsra_vim
                    0x0: VOPIVI::vsra_vim({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_sd[i] = Vs2_sd[i] >> (imm & (64 - 1));
                            }
                        }
                    }}, flags=['SimdShiftOp']);
                }
            }

            // OPIVX
            0x4: decode FUNCT6 {
                // vadd
                0x0: decode VM {
                    // vadd_vx
                    0x1: VOPIVX::vadd_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = Vs2_sd[i] + Rs1_sd;
                        }
                    }}, flags=['SimdAddOp']);

                    // vadd_vxm
                    0x0: VOPIVX::vadd_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_sd[i] = Vs2_sd[i] + Rs1_sd;
                            }
                        }
                    }}, flags=['SimdAddOp']);
                }

                // vsub
                0x2: decode VM {
                    // vsub_vx
                    0x1: VOPIVX::vsub_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = Vs2_sd[i] - Rs1_sd;
                        }
                    }}, flags=['SimdAddOp']);

                    // vsub_vxm
                    0x0: VOPIVX::vsub_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_sd[i] = Vs2_sd[i] - Rs1_sd;
                            }
                        }
                    }}, flags=['SimdAddOp']);
                }

                // vrsub
                0x3: decode VM {
                    // vrsub_vx
                    0x1: VOPIVX::vrsub_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = Rs1_sd - Vs2_sd[i];
                        }
                    }}, flags=['SimdAddOp']);

                    // vrsub_vxm
                    0x0: VOPIVX::vrsub_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_sd[i] = Rs1_sd - Vs2_sd[i];
                            }
                        }
                    }}, flags=['SimdAddOp']);
                }

                // vminu
                0x4: decode VM {
                    // vminu_vx
                    0x1: VOPIVX::vminu_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = std::min(Vs2_ud[i] , Rs1_ud);
                        }
                    }}, flags=['SimdCmpOp']);

                    // vminu_vxm
                    0x0: VOPIVX::vminu_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = std::min(Vs2_ud[i] , Rs1_ud);
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmin
                0x5: decode VM {
                    // vmin_vx
                    0x1: VOPIVX::vmin_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = std::min(Vs2_sd[i] , Rs1_sd);
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmin_vxm
                    0x0: VOPIVX::vmin_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_sd[i] = std::min(Vs2_sd[i] , Rs1_sd);
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmaxu
                0x6: decode VM {
                    // vmaxu_vx
                    0x1: VOPIVX::vmaxu_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = std::max(Vs2_ud[i] , Rs1_ud);
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmaxu_vxm
                    0x0: VOPIVX::vmaxu_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = std::max(Vs2_ud[i] , Rs1_ud);
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmax
                0x7: decode VM {
                    // vmax_vx
                    0x1: VOPIVX::vmax_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = std::max(Vs2_sd[i] , Rs1_sd);
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmax_vxm
                    0x0: VOPIVX::vmax_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_sd[i] = std::max(Vs2_sd[i] , Rs1_sd);
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vand
                0x9: decode VM {
                    // vand_vx
                    0x1: VOPIVX::vand_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] & Rs1_ud;
                        }
                    }}, flags=['SimdAluOp']);

                    // vand_vxm
                    0x0: VOPIVX::vand_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] & Rs1_ud;
                            }
                        }
                    }}, flags=['SimdAluOp']);
                }

                // vor
                0xa: decode VM {
                    // vor_vx
                    0x1: VOPIVX::vor_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] | Rs1_ud;
                        }
                    }}, flags=['SimdAluOp']);

                    // vor_vxm
                    0x0: VOPIVX::vor_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] | Rs1_ud;
                            }
                        }
                    }}, flags=['SimdAluOp']);
                }

                // vxor
                0x0b: decode VM {
                    // vxor_vx
                    0x1: VOPIVX::vxor_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] ^ Rs1_ud;
                        }
                    }}, flags=['SimdAluOp']);

                    // vxor_vxm
                    0x0: VOPIVX::vxor_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] ^ Rs1_ud;
                            }
                        }
                    }}, flags=['SimdAluOp']);
                }

                // vslideup
                0x0e: decode VM {
                    // vslideup_vx
                    0x1: VOPIVX::vslideup_vx({{
                        for (auto i = Rs1_ud; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i - Rs1_ud];
                        }
                    }}, flags=['SimdAluOp']);

                    // vslideup_vxm
                    0x0: VOPIVX::vslideup_vxm({{
                        for (auto i = Rs1_ud; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i - Rs1_ud];
                            }
                        }
                    }}, flags=['SimdAluOp']);
                }

                // vslidedown
                0x0f: decode VM {
                    // vslidedown_vx
                    0x1: VOPIVX::vslidedown_vx({{
                        auto max = xc->maxVectorLength();
                        for (auto i = Rs1_ud; i < VLEN; i++) {
                            if ((i + Rs1_ud) < max) {
                                Vd_ud[i] = Vs2_ud[i + Rs1_ud];
                            } else {
                                Vd_ud[i] = 0;
                            }
                        }
                    }}, flags=['SimdAluOp']);

                    // vslidedown_vxm
                    0x0: VOPIVX::vslidedown_vxm({{
                        auto max = xc->maxVectorLength();
                        for (auto i = Rs1_ud; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                if ((i + Rs1_ud) < max) {
                                    Vd_ud[i] = Vs2_ud[i + Rs1_ud];
                                } else {
                                    Vd_ud[i] = 0;
                                }
                            }
                        }
                    }}, flags=['SimdAluOp']);
                }

                // vmerge/vmv
                0x17: decode VM {
                    // vmv
                    0x1: VOPIVX::vmv_v_x({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Rs1_ud;
                        }
                    }}, flags=['SimdAluOp']);

                    // vmerge_vx
                    0x0: VOPIVX::vmerge_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Rs1_ud;
                            } else {
                                Vd_ud[i] = Vs2_ud[i];
                            }
                        }
                    }}, flags=['SimdAluOp']);
                }

                // vmseq
                0x18: decode VM {
                    // vmseq_vx
                    0x1: VOPIVX::vmseq_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] == Rs1_ud;
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmseq_vxm
                    0x0: VOPIVX::vmseq_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_sd[i] == Rs1_ud;
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmsne
                0x19: decode VM {
                    // vmsne_vx
                    0x1: VOPIVX::vmsne_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] != Rs1_ud;
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmsne_vxm
                    0x0: VOPIVX::vmsne_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] != Rs1_ud;
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmsltu
                0x1a: decode VM {
                    // vmsltu_vx
                    0x1: VOPIVX::vmsltu_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] < Rs1_ud;
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmsltu_vxm
                    0x0: VOPIVX::vmsltu_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] < Rs1_ud;
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmslt
                0x1b: decode VM {
                    // vmslt_vx
                    0x1: VOPIVX::vmslt_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = Vs2_sd[i] < Rs1_sd;
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmslt_vxm
                    0x0: VOPIVX::vmslt_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_sd[i] = Vs2_sd[i] < Rs1_sd;
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmsleu
                0x1c: decode VM {
                    // vmsleu_vx
                    0x1: VOPIVX::vmsleu_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] <= Rs1_ud;
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmsleu_vxm
                    0x0: VOPIVX::vmsleu_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] <= Rs1_ud;
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmsle
                0x1d: decode VM {
                    // vmsle_vx
                    0x1: VOPIVX::vmsle_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = Vs2_sd[i] <= Rs1_sd;
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmsle_vxm
                    0x0: VOPIVX::vmsle_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_sd[i] = Vs2_sd[i] <= Rs1_sd;
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmsgtu
                0x1e: decode VM {
                    // vmsgtu_vx
                    0x1: VOPIVX::vmsgtu_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] >  Rs1_ud;
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmsgtu_vxm
                    0x0: VOPIVX::vmsgtu_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] >  Rs1_ud;
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vmsgt
                0x1f: decode VM {
                    // vmsgt_vx
                    0x1: VOPIVX::vmsgt_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = Vs2_sd[i] >  Rs1_sd;
                        }
                    }}, flags=['SimdCmpOp']);

                    // vmsgt_vxm
                    0x0: VOPIVX::vmsgt_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_sd[i] = Vs2_sd[i] >  Rs1_sd;
                            }
                        }
                    }}, flags=['SimdCmpOp']);
                }

                // vsll
                0x25: decode VM {
                    // vsll_vx
                    0x1: VOPIVX::vsll_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] << (Rs1_ud & (64 - 1));
                        }
                    }}, flags=['SimdShiftOp']);

                    // vsll_vxm
                    0x0: VOPIVX::vsll_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] << (Rs1_ud & (64 - 1));
                            }
                        }
                    }}, flags=['SimdShiftOp']);
                }

                // vsrl
                0x28: decode VM {
                    // vsrl_vx
                    0x1: VOPIVX::vsrl_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = Vs2_ud[i] >> (Rs1_ud & (64 - 1));
                        }
                    }}, flags=['SimdShiftOp']);

                    // vsrl_vxm
                    0x0: VOPIVX::vsrl_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_ud[i] = Vs2_ud[i] >> (Rs1_ud & (64 - 1));
                            }
                        }
                    }}, flags=['SimdShiftOp']);
                }

                // vsra
                0x29: decode VM {
                    // vsra_vx
                    0x1: VOPIVX::vsra_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = Vs2_sd[i] >> (Rs1_sd & (64 - 1));
                        }
                    }}, flags=['SimdShiftOp']);

                    // vsra_vxm
                    0x0: VOPIVX::vsra_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_sd[i] = Vs2_sd[i] >> (Rs1_sd & (64 - 1));
                            }
                        }
                    }}, flags=['SimdShiftOp']);
                }
            }

            // OPFVF
            // TODO doesnt check for NAN in any of these, teehee
            0x5: decode FUNCT6 {
                // vfadd
                0x0: decode VM {
                    // vfadd_vf
                    0x1: VOPFVF::vfadd_vf({{
                        for (auto i = 0; i < VLEN; i++) {
                            uint32_t temp;
                            float rs1 = reinterpret_cast<float&>(temp = Rs1_sd);
                            float fs2 = reinterpret_cast<float&>(temp = Vs2_sd[i]);
                            float res = fs2 + rs1;
                            Vd_sd[i] = reinterpret_cast<uint32_t&>(res);
                        }
                    }}, flags=['SimdFloatAddOp']);

                    // vfadd_vfm
                    0x0: VOPFVF::vfadd_vfm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                uint32_t temp;
                                float rs1 = reinterpret_cast<float&>(temp = Rs1_sd);
                                float fs2 = reinterpret_cast<float&>(temp = Vs2_sd[i]);
                                float res = fs2 + rs1;
                                Vd_sd[i] = reinterpret_cast<uint32_t&>(res);
                            }
                        }
                    }}, flags=['SimdFloatAddOp']);
                }

                // vfsub
                0x2: decode VM {
                    // vfsub_vf
                    0x1: VOPFVF::vfsub_vf({{
                        for (auto i = 0; i < VLEN; i++) {
                            uint32_t temp;
                            float rs1 = reinterpret_cast<float&>(temp = Rs1_sd);
                            float fs2 = reinterpret_cast<float&>(temp = Vs2_sd[i]);
                            float res = fs2 - rs1;
                            Vd_sd[i] = reinterpret_cast<uint32_t&>(res);
                        }
                    }}, flags=['SimdFloatAddOp']);

                    // vfsub_vfm
                    0x0: VOPFVF::vfsub_vfm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                uint32_t temp;
                                float rs1 = reinterpret_cast<float&>(temp = Rs1_sd);
                                float fs2 = reinterpret_cast<float&>(temp = Vs2_sd[i]);
                                float res = fs2 - rs1;
                                Vd_sd[i] = reinterpret_cast<uint32_t&>(res);
                            }
                        }
                    }}, flags=['SimdFloatAddOp']);
                }

                // // vminu
                // 0x4: decode VM {
                //     // vminu_vx
                //     0x1: VOPIVX::vminu_vx({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_ud[i] = std::min(Vs2_ud[i] , Rs1_ud);
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vminu_vxm
                //     0x0: VOPIVX::vminu_vxm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_ud[i] = std::min(Vs2_ud[i] , Rs1_ud);
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // // vmin
                // 0x5: decode VM {
                //     // vmin_vx
                //     0x1: VOPIVX::vmin_vx({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_sd[i] = std::min(Vs2_sd[i] , Rs1_sd);
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vmin_vxm
                //     0x0: VOPIVX::vmin_vxm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_sd[i] = std::min(Vs2_sd[i] , Rs1_sd);
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // // vmaxu
                // 0x6: decode VM {
                //     // vmaxu_vx
                //     0x1: VOPIVX::vmaxu_vx({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_ud[i] = std::max(Vs2_ud[i] , Rs1_ud);
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vmaxu_vxm
                //     0x0: VOPIVX::vmaxu_vxm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_ud[i] = std::max(Vs2_ud[i] , Rs1_ud);
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // // vmax
                // 0x7: decode VM {
                //     // vmax_vx
                //     0x1: VOPIVX::vmax_vx({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_sd[i] = std::max(Vs2_sd[i] , Rs1_sd);
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vmax_vxm
                //     0x0: VOPIVX::vmax_vxm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_sd[i] = std::max(Vs2_sd[i] , Rs1_sd);
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // vfslide1up
                0x0e: decode VM {
                    // vfslide1up_vf
                    0x1: VOPFVF::vfslide1up_vf({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (i == 0) {
                                uint32_t temp;
                                float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                                DPRINTF(RiscvVector, "slide in %f\n", fs1);
                                Vd_ud[i] = reinterpret_cast<uint32_t&>(fs1);
                            } else {
                                Vd_ud[i] = Vs2_ud[i - 1];
                            }
                        }
                    }}, flags=['SimdFloatAluOp']);

                    // vfslide1up_vfm
                    0x0: VOPFVF::vfslide1up_vfm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                if (i == 0) {
                                    uint32_t temp;
                                    float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                                    Vd_ud[i] = reinterpret_cast<uint32_t&>(fs1);
                                } else {
                                    Vd_ud[i] = Vs2_ud[i - 1];
                                }
                            }
                        }
                    }}, flags=['SimdFloatAluOp']);
                }

                // vfslide1down
                0x0f: decode VM {
                    // vfslide1down_vf
                    0x1: VOPFVF::vfslide1down_vf({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (i == (VLEN - 1)) {
                                uint32_t temp;
                                float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                                Vd_ud[i] = reinterpret_cast<uint32_t&>(fs1);
                            } else {
                                Vd_ud[i] = Vs2_ud[i + 1];
                            }
                        }
                    }}, flags=['SimdFloatAluOp']);

                    // vfslide1down_vxm
                    0x0: VOPFVF::vfslide1down_vfm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                if (i == (VLEN - 1)) {
                                    uint32_t temp;
                                    float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                                    Vd_ud[i] = reinterpret_cast<uint32_t&>(fs1);
                                } else {
                                    Vd_ud[i] = Vs2_ud[i + 1];
                                }
                            }
                        }
                    }}, flags=['SimdFloatAluOp']);
                }

                // vmv
                0x10: decode VM {
                    // vmv
                    0x1: VOPFVF::vfmv_s_f({{
                        uint32_t temp;
                        float fs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                        Vd_ud[0] = reinterpret_cast<uint32_t&>(fs1);
                    }}, flags=['SimdFloatAluOp']);
                }


                // vmerge/vmv
                0x17: decode VM {
                    // vmv
                    0x1: VOPFVF::vfmv_v_f({{
                        uint32_t temp;
                        float rs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_ud[i] = reinterpret_cast<uint32_t&>(rs1);
                        }
                    }}, flags=['SimdFloatAluOp']);

                    // // vmerge_vx
                    // 0x0: VOPIVX::vmerge_vxm({{
                    //     for (auto i = 0; i < VLEN; i++) {
                    //         if (Vm_ud[i] & 0x1llu) {
                    //             Vd_ud[i] = Rs1_ud;
                    //         } else {
                    //             Vd_ud[i] = Vs2_ud[i];
                    //         }
                    //     }
                    // }}, flags=['SimdAluOp']);
                }

                // // vmseq
                // 0x18: decode VM {
                //     // vmseq_vx
                //     0x1: VOPIVX::vmseq_vx({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_ud[i] = Vs2_ud[i] == Rs1_ud;
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vmseq_vxm
                //     0x0: VOPIVX::vmseq_vxm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_ud[i] = Vs2_sd[i] == Rs1_ud;
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // // vmsne
                // 0x19: decode VM {
                //     // vmsne_vx
                //     0x1: VOPIVX::vmsne_vx({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_ud[i] = Vs2_ud[i] != Rs1_ud;
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vmsne_vxm
                //     0x0: VOPIVX::vmsne_vxm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_ud[i] = Vs2_ud[i] != Rs1_ud;
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // // vmsltu
                // 0x1a: decode VM {
                //     // vmsltu_vx
                //     0x1: VOPIVX::vmsltu_vx({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_ud[i] = Vs2_ud[i] < Rs1_ud;
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vmsltu_vxm
                //     0x0: VOPIVX::vmsltu_vxm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_ud[i] = Vs2_ud[i] < Rs1_ud;
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // // vmslt
                // 0x1b: decode VM {
                //     // vmslt_vx
                //     0x1: VOPIVX::vmslt_vx({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_sd[i] = Vs2_sd[i] < Rs1_sd;
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vmslt_vxm
                //     0x0: VOPIVX::vmslt_vxm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_sd[i] = Vs2_sd[i] < Rs1_sd;
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // // vmsleu
                // 0x1c: decode VM {
                //     // vmsleu_vx
                //     0x1: VOPIVX::vmsleu_vx({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_ud[i] = Vs2_ud[i] <= Rs1_ud;
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vmsleu_vxm
                //     0x0: VOPIVX::vmsleu_vxm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_ud[i] = Vs2_ud[i] <= Rs1_ud;
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // // vmsle
                // 0x1d: decode VM {
                //     // vmsle_vx
                //     0x1: VOPIVX::vmsle_vx({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_sd[i] = Vs2_sd[i] <= Rs1_sd;
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vmsle_vxm
                //     0x0: VOPIVX::vmsle_vxm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_sd[i] = Vs2_sd[i] <= Rs1_sd;
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // // vmsgtu
                // 0x1e: decode VM {
                //     // vmsgtu_vx
                //     0x1: VOPIVX::vmsgtu_vx({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_ud[i] = Vs2_ud[i] >  Rs1_ud;
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vmsgtu_vxm
                //     0x0: VOPIVX::vmsgtu_vxm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_ud[i] = Vs2_ud[i] >  Rs1_ud;
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // // vmsgt
                // 0x1f: decode VM {
                //     // vmsgt_vx
                //     0x1: VOPIVX::vmsgt_vx({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             Vd_sd[i] = Vs2_sd[i] >  Rs1_sd;
                //         }
                //     }}, flags=['SimdCmpOp']);

                //     // vmsgt_vxm
                //     0x0: VOPIVX::vmsgt_vxm({{
                //         for (auto i = 0; i < VLEN; i++) {
                //             if (Vm_ud[i] & 0x1llu) {
                //                 Vd_sd[i] = Vs2_sd[i] >  Rs1_sd;
                //             }
                //         }
                //     }}, flags=['SimdCmpOp']);
                // }

                // vfdiv
                0x20: decode VM {
                    // vfdiv_vf
                    0x1: VOPFVF::vfdiv_vf({{
                        for (auto i = 0; i < VLEN; i++) {
                            uint32_t temp;
                            float rs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                            float fs2 = reinterpret_cast<float&>(temp = Vs2_sd[i]);
                            float res = fs2 / rs1;
                            DPRINTF(RiscvVector, "%f = %f / %f\n", res, fs2, rs1);
                            Vd_sd[i] = reinterpret_cast<uint32_t&>(res);
                        }
                    }}, flags=['SimdFloatDivOp']);

                    // vfdiv_vfm
                    0x0: VOPFVF::vfdiv_vfm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                uint32_t temp;
                                float rs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                                float fs2 = reinterpret_cast<float&>(temp = Vs2_sd[i]);
                                float res = fs2 / rs1;
                                Vd_sd[i] = reinterpret_cast<uint32_t&>(res);
                            }
                        }
                    }}, flags=['SimdFloatDivOp']);
                }

                // vfrdiv (reverse div order)
                0x21: decode VM {
                    // vfrdiv_vf
                    0x1: VOPFVF::vfrdiv_vf({{
                        for (auto i = 0; i < VLEN; i++) {
                            uint32_t temp;
                            float rs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                            float fs2 = reinterpret_cast<float&>(temp = Vs2_sd[i]);
                            float res = rs1 / fs2;
                            Vd_sd[i] = reinterpret_cast<uint32_t&>(res);
                        }
                    }}, flags=['SimdFloatDivOp']);

                    // vfdiv_vfm
                    0x0: VOPFVF::vfrdiv_vfm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                uint32_t temp;
                                float rs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                                float fs2 = reinterpret_cast<float&>(temp = Vs2_sd[i]);
                                float res = rs1 / fs2;
                                Vd_sd[i] = reinterpret_cast<uint32_t&>(res);
                            }
                        }
                    }}, flags=['SimdFloatDivOp']);
                }

                // vfmul
                0x24: decode VM {
                    // vfmul_vf
                    0x1: VOPFVF::vfmul_vf({{
                        for (auto i = 0; i < VLEN; i++) {
                            uint32_t temp;
                            float rs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                            float fs2 = reinterpret_cast<float&>(temp = Vs2_sd[i]);
                            float res = fs2 * rs1;
                            DPRINTF(RiscvVector, "%f = %f * %f\n", res, fs2, rs1);
                            Vd_sd[i] = reinterpret_cast<uint32_t&>(res);
                        }
                    }}, flags=['SimdFloatMultOp']);

                    // vfmul_vfm
                    0x0: VOPFVF::vfmul_vfm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                uint32_t temp;
                                float rs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                                float fs2 = reinterpret_cast<float&>(temp = Vs2_sd[i]);
                                float res = fs2 * rs1;
                                Vd_sd[i] = reinterpret_cast<uint32_t&>(res);
                            }
                        }
                    }}, flags=['SimdFloatMultOp']);
                }

                // vfrsub
                0x27: decode VM {
                    // vfrsub_vf
                    0x1: VOPFVF::vfrsub_vf({{
                        for (auto i = 0; i < VLEN; i++) {
                            uint32_t temp;
                            float rs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                            float fs2 = reinterpret_cast<float&>(temp = Vs2_sd[i]);
                            float res = rs1 - fs2;
                            Vd_sd[i] = reinterpret_cast<uint32_t&>(res);
                        }
                    }}, flags=['SimdFloatAddOp']);

                    // vfrsub_vfm
                    0x0: VOPFVF::vfrsub_vfm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                uint32_t temp;
                                float rs1 = reinterpret_cast<float&>(temp = Fs1_bits);
                                float fs2 = reinterpret_cast<float&>(temp = Vs2_sd[i]);
                                float res = rs1 - fs2;
                                Vd_sd[i] = reinterpret_cast<uint32_t&>(res);
                            }
                        }
                    }}, flags=['SimdFloatAddOp']);
                }
            }

            // OPMVX
            0x6: decode FUNCT6 {
                0x10: decode VS2 {
                    0x00: VOPIVX::vmv_s_x({{
                        Vd_ud[0] = Rs1_ud;
                    }}, flags=['SimdAluOp']);
                }

                // vslide1up
                0x0e: decode VM {
                    // vslide1up_vx
                    0x1: VOPIVX::vslide1up_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (i == 0) {
                                Vd_ud[i] = Rs1_ud;
                            } else {
                                Vd_ud[i] = Vs2_ud[i - 1];
                            }
                        }
                    }}, flags=['SimdAluOp']);

                    // vslide1up_vxm
                    0x0: VOPIVX::vslide1up_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                if (i == 0) {
                                    Vd_ud[i] = Rs1_ud;
                                } else {
                                    Vd_ud[i] = Vs2_ud[i - 1];
                                }
                            }
                        }
                    }}, flags=['SimdAluOp']);
                }

                // vslide1down
                0x0f: decode VM {
                    // vslide1down_vx
                    0x1: VOPIVX::vslide1down_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (i == (VLEN - 1)) {
                                Vd_ud[i] = Rs1_ud;
                            } else {
                                Vd_ud[i] = Vs2_ud[i + 1];
                            }
                        }
                    }}, flags=['SimdAluOp']);

                    // vslide1down_vxm
                    0x0: VOPIVX::vslide1down_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                if (i == (VLEN - 1)) {
                                    Vd_ud[i] = Rs1_ud;
                                } else {
                                    Vd_ud[i] = Vs2_ud[i + 1];
                                }
                            }
                        }
                    }}, flags=['SimdAluOp']);
                }

                // vmul
                0x25: decode VM {
                    // vmul_vx
                    0x1: VOPIVX::vmul_vx({{
                        for (auto i = 0; i < VLEN; i++) {
                            Vd_sd[i] = Vs2_sd[i] * Rs1_sd;
                        }
                    }}, flags=['SimdMultOp']);

                    // vmul_vxm
                    0x0: VOPIVX::vmul_vxm({{
                        for (auto i = 0; i < VLEN; i++) {
                            if (Vm_ud[i] & 0x1llu) {
                                Vd_sd[i] = Vs2_sd[i] * Rs1_sd;
                            }
                        }
                    }}, flags=['SimdMultOp']);
                }
            }

            // vsetvl
            0x7: decode VMSB {
                0x1: decode VSETVL {
                    // setvl
                    0x0: VCSROp::vsetvl({{
                        auto max = xc->maxVectorLength();
                        auto rVL = (RS1 == 0)? max: Rs1_ud;
                        auto gVL = (rVL > max)? max: rVL;
                        VLEN = gVL;
                        Rd = gVL;
                    }}, flags=['SimdMiscOp']);
                }

                // setvli
                0x0: VCSROp::vsetvli({{
                    auto max = xc->maxVectorLength();
                    auto rVL = (RS1 == 0)? max: Rs1_ud;
                    auto gVL = (rVL > max)? max: rVL;
                    VLEN = gVL;
                    Rd = gVL;
                }}, flags=['SimdMiscOp']);
            }
        }

        0x18: decode FUNCT3 {
            format BOp {
                0x0: beq({{
                    if (Rs1 == Rs2) {
                        NPC = PC + imm;
                    } else {
                        NPC = NPC;
                    }
                }}, IsDirectControl, IsCondControl);
                0x1: bne({{
                    if (Rs1 != Rs2) {
                        NPC = PC + imm;
                    } else {
                        NPC = NPC;
                    }
                }}, IsDirectControl, IsCondControl);
                0x4: blt({{
                    if (Rs1_sd < Rs2_sd) {
                        NPC = PC + imm;
                    } else {
                        NPC = NPC;
                    }
                }}, IsDirectControl, IsCondControl);
                0x5: bge({{
                    if (Rs1_sd >= Rs2_sd) {
                        NPC = PC + imm;
                    } else {
                        NPC = NPC;
                    }
                }}, IsDirectControl, IsCondControl);
                0x6: bltu({{
                    if (Rs1 < Rs2) {
                        NPC = PC + imm;
                    } else {
                        NPC = NPC;
                    }
                }}, IsDirectControl, IsCondControl);
                0x7: bgeu({{
                    if (Rs1 >= Rs2) {
                        NPC = PC + imm;
                    } else {
                        NPC = NPC;
                    }
                }}, IsDirectControl, IsCondControl);
            }
        }

        0x19: decode FUNCT3 {
            0x0: Jump::jalr({{
                Rd = NPC;
                NPC = (imm + Rs1) & (~0x1);
            }}, IsIndirectControl, IsUncondControl, IsCall);
        }

        0x1b: JOp::jal({{
            Rd = NPC;
            NPC = PC + imm;
        }}, IsDirectControl, IsUncondControl, IsCall);

        0x1c: decode FUNCT3 {
            format SystemOp {
                0x0: decode FUNCT12 {
                    0x0: ecall({{
                        fault = make_shared<SyscallFault>(
                                (PrivilegeMode)xc->readMiscReg(MISCREG_PRV));
                    }}, IsSerializeAfter, IsNonSpeculative, IsSyscall,
                        No_OpClass);
                    0x1: ebreak({{
                        fault = make_shared<BreakpointFault>(xc->pcState());
                    }}, IsSerializeAfter, IsNonSpeculative, No_OpClass);
                    0x2: uret({{
                        STATUS status = xc->readMiscReg(MISCREG_STATUS);
                        status.uie = status.upie;
                        status.upie = 1;
                        xc->setMiscReg(MISCREG_STATUS, status);
                        NPC = xc->readMiscReg(MISCREG_UEPC);
                    }}, IsReturn);
                    0x102: sret({{
                        if (xc->readMiscReg(MISCREG_PRV) == PRV_U) {
                            fault = make_shared<IllegalInstFault>(
                                        "sret in user mode", machInst);
                            NPC = NPC;
                        } else {
                            STATUS status = xc->readMiscReg(MISCREG_STATUS);
                            xc->setMiscReg(MISCREG_PRV, status.spp);
                            status.sie = status.spie;
                            status.spie = 1;
                            status.spp = PRV_U;
                            xc->setMiscReg(MISCREG_STATUS, status);
                            NPC = xc->readMiscReg(MISCREG_SEPC);
                        }
                    }}, IsReturn);
                    0x302: mret({{
                        if (xc->readMiscReg(MISCREG_PRV) != PRV_M) {
                            fault = make_shared<IllegalInstFault>(
                                        "mret at lower privilege", machInst);
                            NPC = NPC;
                        } else {
                            STATUS status = xc->readMiscReg(MISCREG_STATUS);
                            xc->setMiscReg(MISCREG_PRV, status.mpp);
                            status.mie = status.mpie;
                            status.mpie = 1;
                            status.mpp = PRV_U;
                            xc->setMiscReg(MISCREG_STATUS, status);
                            NPC = xc->readMiscReg(MISCREG_MEPC);
                        }
                    }}, IsReturn);
                }
            }
            format CSROp {
                0x1: csrrw({{
                    Rd = data;
                    data = Rs1;
                }}, IsNonSpeculative, No_OpClass);
                0x2: csrrs({{
                    Rd = data;
                    data |= Rs1;
                }}, IsNonSpeculative, No_OpClass);
                0x3: csrrc({{
                    Rd = data;
                    data &= ~Rs1;
                }}, IsNonSpeculative, No_OpClass);
                0x5: csrrwi({{
                    Rd = data;
                    data = uimm;
                }}, IsNonSpeculative, No_OpClass);
                0x6: csrrsi({{
                    Rd = data;
                    data |= uimm;
                }}, IsNonSpeculative, No_OpClass);
                0x7: csrrci({{
                    Rd = data;
                    data &= ~uimm;
                }}, IsNonSpeculative, No_OpClass);
            }
        }
        
        // custom ops for 0b11
        // free up until 0x1f
        // 0x02
        // 0x07
        // 0x0a
        // 0x0f
        // 0x1a
        // 0x1d
        // 0x1e
        // could use a different encoding then 0b11 to start in real impl, 
        // but then .insn directive wouldn't work (enforces 0b11 start)!
        
        //0x1a: BUOp::csrexe({{
            // add a new instruction that takes a 20bit immediate
            // and writes it to a specific csr
            // imm is defined in BUOP format
            //printf("imm val: %ld\n", imm);
        //    xc->setMiscReg(MISCREG_EXE, imm);
        //}}, IsNonSpeculative, IsBind, No_OpClass);
        
        0x1d: CSROp::csrfet({{
            Rd = data;
            data = Rs1;
            //xc->setMiscReg(MISCREG_FETCH, imm);
            //printf("csrfetch %ld\n", imm);
        //IsSerializeAfter, IsNonSpeculative, IsBind, No_OpClass);
        }}, IsSquashAfter, IsNonSpeculative, IsBind, No_OpClass);
        
        0x1e: BUOp::revec ({{
            // nada
        }}, IsRevec, IsNonSpeculative, No_OpClass);

        // load 20 bits into lower reg
        //0x1e: BUOp::lli({{
        //    Rd = imm;
        //}})

        //0x02: BUOp::remem ({{
        //    // nada
        //}}, IsRemem, IsNonSpeculative, No_OpClass);

        0x1a: JOp::vissue({{
            // note the compiler is smart enough to mark which resources you use based on this description
            // so if don't include any registers then won't mark compiled instructions as needing to use any
            // see build/RVSP/arch/riscv/generated/decoder-ns.cc.inc
            NPC = PC + imm;
        }}, IsVectorIssue, No_OpClass);

        0x0a: JOp::devec({{
            // set PC to specified PC and take out of vec mode
            NPC = PC + imm;
            xc->setMiscReg(MISCREG_FETCH, 0);
        }}, IsSquashAfter, IsVectorIssue, No_OpClass);

        
    }
}
